{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYMyRHDpRVHp",
    "outputId": "6bb75a20-bd90-48f1-9b77-ca410522352f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 265 µs (started: 2022-11-18 18:38:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUSQ0_cQZ_EG",
    "tags": []
   },
   "source": [
    "# Cargar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f7908434e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.22 s (started: 2022-11-18 18:38:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# imporar librerias\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms \n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet101\n",
    "import urllib.request\n",
    "import cv2\n",
    "\n",
    "from ignite.metrics import ClassificationReport\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "\n",
    "from genericpath import exists\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91.3 ms (started: 2022-11-18 18:38:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.84 s (started: 2022-11-18 18:38:44 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnext101_32x8d\n",
    "resnext1 = resnext101_32x8d(pretrained=True)\n",
    "resnext2 = resnext101_32x8d(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.03 ms (started: 2022-11-18 18:38:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Cargar backbone resnet y resnext\n",
    "class MultilabelClassifier1(nn.Module):\n",
    "    def __init__(self, n_classes, pretrain_model):\n",
    "        super().__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.pretrain_model.children())[:-1]))\n",
    "\n",
    "        self.classes = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=2048, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'label': self.classes(x)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.23 s (started: 2022-11-18 18:38:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo para cara y n° domicilio\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "PATH_1 =  '/home/jupyter/Score/Modelos/mlc_model_4.pth'\n",
    "ml_model_1 = MultilabelClassifier1(3, resnext1).to(device)\n",
    "ml_model_dict_1 = torch.load(PATH_1, map_location=torch.device(device))\n",
    "ml_model_1.load_state_dict(ml_model_dict_1['model_state_dict'])\n",
    "#torch.save(ml_model_1.load_state_dict(ml_model_dict_1['model_state_dict']), '/home/jupyter/Score/Modelos/mlc_state_dict_4.pt')\n",
    "#ml_model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.648120448645789"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.12 ms (started: 2022-11-18 18:38:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ml_model_dict_1['validation loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "time: 1.28 ms (started: 2022-11-18 18:38:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# obtener epoca óptima\n",
    "optimal_epoch = ml_model_dict_1['epoch']\n",
    "print(optimal_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.44 ms (started: 2022-11-18 18:38:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Cargar backbone -> para modelo 'mlc_model_baseline_data_revisada_1_2.pth'\n",
    "class MultilabelClassifier2(nn.Module):\n",
    "    def __init__(self, n_classes, pretrain_model):\n",
    "        super().__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.pretrain_model.children())[:-1]))\n",
    "\n",
    "        self.classes = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=2048, out_features=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'label': self.classes(x)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 789 ms (started: 2022-11-18 18:38:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo para etiqueta del producto\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "PATH_2 = '/home/jupyter/Score/Modelos/mlc_model_baseline_data_revisada_1_2.pth'\n",
    "ml_model_2 = MultilabelClassifier2(3, resnext2).to(device)\n",
    "ml_model_dict_2 = torch.load(PATH_2, map_location=torch.device(device))\n",
    "ml_model_2.load_state_dict(ml_model_dict_2['model_state_dict'])\n",
    "#torch.save(ml_model_2.load_state_dict(ml_model_dict_2['model_state_dict']), '/home/jupyter/Score/Modelos/mlc_state_dict_baseline_data_revisada_1_2.pt')\n",
    "#ml_model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0754197467943256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.75 ms (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ml_model_dict_2['validation loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "time: 1.2 ms (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# obtener epoca óptima\n",
    "optimal_epoch = ml_model_dict_2['epoch']\n",
    "print(optimal_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 927 µs (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# REFERENCIAR!\n",
    "# función de ayuda para cargar modelo\n",
    "def get_object_detection_model(num_classes):\n",
    "  # load a model pre-trained on COCO\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  # get number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 842 ms (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo detector de objetos\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "model_path = '/home/jupyter/Score/Modelos/model_cf_12y3_1.pth'\n",
    "num_classes = 2\n",
    "od_model = get_object_detection_model(num_classes)\n",
    "od_model_dict = torch.load(model_path, map_location=torch.device(device))\n",
    "od_model.load_state_dict(od_model_dict['model_state_dict'])\n",
    "#torch.save(od_model.load_state_dict(od_model_dict['model_state_dict']), '/home/jupyter/Score/Modelos/state_dict_cf_12y3_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.5 ms (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53168\n",
      "time: 3.11 s (started: 2022-11-18 18:38:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Query para obtener la data de 3 días antes, considerando que el enlace de la foto, \n",
    "# el nombre del transportista y su rut, no sean nulos\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT DISTINCT transport_ord_id as SOC, i.url as url, shipment.plate_num as plate_num, \n",
    "provider.doc_id as provider_id, \n",
    "provider.doc_verify_digit as provider_verify_digit,\n",
    "provider.name as provider_name, driver.doc_id as driver_id, \n",
    "driver.doc_verify_digit as driver_verify_digit,\n",
    "driver.name as driver_name, driver.last_name as driver_last_name,\n",
    "DATETIME(event_crte_tmst, 'America/Santiago') as event_crte_tmst, dfl_crte_tmst\n",
    "FROM \n",
    "`tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.btd_scha_fal_trmg_api_transport_order_temp`,\n",
    "unnest(image) as i\n",
    " \n",
    "WHERE\n",
    "  i.url is not null\n",
    "  and provider.name is not null\n",
    "  and provider.doc_id is not null\n",
    "  and DATE(event_crte_tmst, 'America/Santiago') = current_date() - 3\n",
    "\"\"\"\n",
    "\n",
    "df_images = client.query(sql).to_dataframe()\n",
    "\n",
    "print(len(df_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<multiprocessing.pool.IMapUnorderedIterator at 0x7f780cc88550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 ms (started: 2022-11-18 18:39:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# eliminar filas con urls duplicados\n",
    "df_images = df_images.drop_duplicates(['url'])\n",
    "# subir imagenes a bucket\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import urllib\n",
    "import urllib.request\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "date = datetime.datetime.now()    \n",
    "def get_data(url):\n",
    "    try:\n",
    "        url_str = url.split('/')[-1]\n",
    "        url_open = urllib.request.urlopen(url)\n",
    "        \n",
    "        image_cv = np.asarray(bytearray(url_open.read()), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_str = cv2.imencode('.png', image)[1].tostring()\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket('pod_images')\n",
    "        # el cero es para distinguir \n",
    "        #blob = bucket.blob(f'{date.year}{date.month}{date.day}/{url_str}')\n",
    "        blob = bucket.blob(f'{2022}{11}0{17}/{url_str}')\n",
    "        blob.upload_from_string(img_str)\n",
    "    except:\n",
    "        # ver el link que tira error\n",
    "        print(url)\n",
    "#urls = df_images.loc[:,'url'].iloc[:10000]\n",
    "urls = df_images.loc[:,'url'].iloc[:1000]\n",
    "cpus = cpu_count()\n",
    "results = ThreadPool(cpus-1).imap_unordered(get_data, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetear indices\n",
    "df_images = df_images.reset_index()\n",
    "# eliminar columna index\n",
    "df_images = df_images.drop(['index'], axis = 1)\n",
    "print(len(df_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 556 µs (started: 2022-11-18 17:56:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas para leer enlaces mas rapido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 318 µs (started: 2022-11-18 17:56:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#inputs_df = df_images.iloc[0:100].url\n",
    "#inputs_list = df_images.iloc[0:10].url.to_list()\n",
    "#inputs_array = df_images.iloc[0:10].url.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47198\n",
      "time: 4.2 s (started: 2022-11-18 18:33:19 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import cv2\n",
    "from tempfile import TemporaryFile\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "date=datetime.datetime.now()\n",
    "\n",
    "#nombre carpeta\n",
    "#folder = str(date.year) + str(date.month) + str(date.day)\n",
    "folder = '20221118'\n",
    "\n",
    "# inicialización\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('pod_images')\n",
    "#blobs = client.list_blobs(bucket)\n",
    "blobs = bucket.list_blobs(prefix=folder)\n",
    "# generar lista de fotos\n",
    "blob_list = list(blobs)\n",
    "print(len(blob_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.14 ms (started: 2022-11-18 17:37:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import cv2\n",
    "from tempfile import TemporaryFile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# función para leer imágenes desde el bucket\n",
    "def read_image_bucket(blob_object):\n",
    "    filename = blob_object.name\n",
    "    imagename = filename.split('/')[1]\n",
    "\n",
    "    client = storage.Client()\n",
    "    \n",
    "    #bucket = client.get_bucket('image_recognition_images')\n",
    "    bucket = client.get_bucket('pod_images')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    \n",
    "    try:\n",
    "        b = blob.download_as_bytes()\n",
    "        image_cv = np.asarray(bytearray(b), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        area = image.shape[0]*image.shape[1]\n",
    "        url = 'https://prdadessacorptrl.blob.core.windows.net/cl-images/' + imagename\n",
    "        #print(image.shape)\n",
    "        #print(area)\n",
    "    except:\n",
    "        image = np.zeros((480,480,3), np.uint8)\n",
    "        area = image.shape[0]*image.shape[1]\n",
    "        url = 'https://prdadessacorptrl.blob.core.windows.net/cl-images/' + imagename\n",
    "        \n",
    "    return(image, area, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 733 µs (started: 2022-11-18 17:37:32 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# funcion para trabajar en paralelo\n",
    "#200.000 imagenes aprox 3 horas\n",
    "# se podria agregar un log cada vez que lee 1000 images y\n",
    "# además ir guardando la lista, de manera de que si se corta\n",
    "# el proceso, se puede retomar desde donde que quedo\n",
    "def open_parallel_bucket(args):\n",
    "    cpus = cpu_count()\n",
    "    #results = ThreadPool(cpus - 1).imap_unordered(read_image_bucket, args)\n",
    "    results = ThreadPool(cpus-1).imap_unordered(read_image_bucket, args)\n",
    "    import numpy as np\n",
    "    lista_base = []\n",
    "    for result in results:\n",
    "        lista_base.append(result)\n",
    "    return(lista_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 30s (started: 2022-11-18 17:37:32 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# leer fotos\n",
    "t_ini = time.time()\n",
    "l_bucket = open_parallel_bucket(blob_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generar dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.39 ms (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Dataset class para generar los batches\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"Photos prediction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, lista_fotos, normalization = 'mlc', device = 'cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lista_fotos (list): Lista con varias fotos concatenadas y sus areas\n",
    "            normalization (string): Indica si se debe implementar la transformación\n",
    "            de normalización para el clasificador multi-etiqueta ('mlc') o para el \n",
    "            detector de objetos ('ob')\n",
    "            device (string): Indica qué dispositivo se quiere usar: 'cuda' o 'cpu'\n",
    "        \n",
    "        Returns:\n",
    "            pro_img (tensor): Retorna la imagen procesada como tensor lista para la predicción.\n",
    "        \"\"\"\n",
    "        self.lista_fotos = lista_fotos\n",
    "        self.normalization = normalization\n",
    "        self.device = device\n",
    "        # transformación para clasificador\n",
    "        self.mlc_normalization = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                                 ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lista_fotos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        foto = self.lista_fotos[idx][0]\n",
    "        area = self.lista_fotos[idx][1]\n",
    "        url = self.lista_fotos[idx][2]\n",
    "        # aplicar normalizacion\n",
    "        if self.normalization == 'mlc':\n",
    "            pro_img  = self.mlc_normalization(foto)\n",
    "            #print(t_img.shape)\n",
    "            #pro_img = t_img.unsqueeze(dim = 0)\n",
    "            #print(pro_img.shape)\n",
    "        \n",
    "        elif self.normalization == 'od':\n",
    "            t_img_0 = transform.resize(foto, (480, 480))\n",
    "            pro_img = torch.Tensor(t_img_0).permute(2,0,1)\n",
    "\n",
    "        # retornar imagen procesada\n",
    "        return pro_img, area, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.63 ms (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# se generan batch para clasificador multi-etiqueta y detector de objetos\n",
    "prediction_set_mlc = PredictionDataset(lista_fotos = l_bucket, normalization = 'mlc', device = 'cuda')\n",
    "prediction_set_od = PredictionDataset(lista_fotos = l_bucket, normalization = 'od', device = 'cuda')\n",
    "\n",
    "# para aumentar más el batch_size hay que agregar más memoria\n",
    "predictionLoader_mlc = DataLoader(prediction_set_mlc, batch_size=32, num_workers=2,\n",
    "                         shuffle = False)\n",
    "predictionLoader_od = DataLoader(prediction_set_od, batch_size=32, num_workers=2,\n",
    "                         shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 898 µs (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import urllib\n",
    "import math\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.26 ms (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def bbox_function(b, area_img):\n",
    "    l =list(range(b.shape[0]))\n",
    "    # recorrer cajas y calcular inter-area para cada combinación\n",
    "    indice = 0\n",
    "    interArea = 0\n",
    "    for i in itertools.combinations(l, r=2):\n",
    "        # obtener coordenadas de la inter-area\n",
    "        x0 = max(b[i[0]][0], b[i[1]][0])\n",
    "        y0 = max(b[i[0]][1], b[i[1]][1])\n",
    "        x1 = min(b[i[0]][2], b[i[1]][2])\n",
    "        y1 = min(b[i[0]][3], b[i[1]][3])\n",
    "\n",
    "        # calcular inter-area\n",
    "        dif_x = x0-x1\n",
    "        dif_y = y0-y1\n",
    "        # se verifica que las esquinas de la interArea esten bien ubicadas \n",
    "        if dif_x < 0 and dif_y < 0:\n",
    "            interArea += dif_x*dif_y\n",
    "            ##interArea += abs(x0-x1)*abs(y0-y1)\n",
    "    \n",
    "    # sumar areas de cada bbox\n",
    "    area_total_bbox = 0\n",
    "    for box in b:\n",
    "        # calcular area de cada bbox\n",
    "        area_bbox = abs(box[0]-box[2])*abs(box[1]-box[3])\n",
    "        # calcular area total de bbox\n",
    "        area_total_bbox += area_bbox\n",
    "\n",
    "    # calcular la union de las areas de cada bbox\n",
    "    union = area_total_bbox - interArea\n",
    "\n",
    "    # calcular contexto\n",
    "    if torch.is_tensor(union):\n",
    "        contexto = union.item()/area_img\n",
    "    \n",
    "    else:\n",
    "        contexto = union/area_img\n",
    "    \n",
    "    return contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.71 ms (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "# supuesto: tabla con enlaces a cada fotografía\n",
    "\n",
    "def score(predictionLoader_mlc, predictionLoader_od, classificator_1, classificator_2, \n",
    "          detector = od_model,pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, \n",
    "                                       'w_contx': 0.2},\n",
    "          thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "                        't_ctx_up': 0.65}, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classificator_1 (modelo): clasificador multi-etiqueta para detectar la etiqueta del paquete\n",
    "        classificator_2 (modelo): clasificador multi-etiqueta para detectar la cara y el domicilio\n",
    "        detector (modelo): detector de objetos para identificar el paquete y su bbox respectivo\n",
    "        pesos (dict): diccionario con valores para cada peso, es decir,\n",
    "        w_prod (product), w_notface (without face), w_label (product label), \n",
    "        w_num (address number) y w_contx (context)\n",
    "        thresholds = diccionario con valores de umbral para cada criterio, es decir,\n",
    "        t_prod (product), t_face (face detector), t_label (product label) y \n",
    "        t_num (address number)\n",
    "        \n",
    "    Obs: los pesos deben sumar 1 para todos los criterios menos el del numero de domicilio. Este\n",
    "        ultimo corresponde a un beneficio de +0.1 si es que aparece en la fotografía.\n",
    "\n",
    "    Returns:\n",
    "        result_data (dataFrame): cada una de las columnas del dataFrame corresponde a la predicción\n",
    "        de cada criterio sobre cierto umbral, los scores (confianza del modelo) de cada criterio y \n",
    "        la nota de la foto (score).\n",
    "    \"\"\"\n",
    "\n",
    "    #pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2}\n",
    "    \n",
    "    #thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "    #              't_ctx_up': 0.65}\n",
    "    \n",
    "    #device = 'cuda'\n",
    "    # evaluar con gpu o cpu \n",
    "    device = torch.device(device)\n",
    "    #classificator_1 = ml_model_1 \n",
    "    #classificator_2 = ml_model_2\n",
    "    #detector = od_model\n",
    "    \n",
    "    classificator_1.to(device)\n",
    "    classificator_1.eval()  \n",
    "    classificator_2.to(device)\n",
    "    classificator_2.eval()           \n",
    "    detector.to(device)\n",
    "    detector.eval()\n",
    "    df_base = pd.DataFrame(columns = ['url','paquete', 's_paquete', 'etiqueta_producto',\n",
    "                                          's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\n",
    "                                          'numero_domicilio', 's_numero_domicilio', 'contexto',\n",
    "                                          'ctx_value', 'score'])\n",
    "    with torch.no_grad():\n",
    "        for i, image in enumerate(zip(predictionLoader_mlc, predictionLoader_od)):\n",
    "            # se obtiene la imagen de cada dataLoader\n",
    "            image_mlc = image[0][0]\n",
    "            image_od = image[1][0]\n",
    "            \n",
    "            # obtener areas y urls de las imagenes\n",
    "            area_img = image[0][1]\n",
    "            #img_url = np.asarray(image[0][2]).reshape(-1,1)\n",
    "            img_url = list(image[0][2])\n",
    "    \n",
    "            # CLASIFICACION MULTI-ETIQUETA\n",
    "            #---------------------------clasificador 1--------------------------------\n",
    "            label_base = classificator_1(image_mlc.to(device))\n",
    "            #---------------------------clasificador 2--------------------------------\n",
    "            label_etiqueta = classificator_2(image_mlc.to(device))\n",
    "            # obtener score\n",
    "            score_etiqueta = torch.sigmoid(label_etiqueta['label']).squeeze()\n",
    "            score = torch.sigmoid(label_base['label']).squeeze()\n",
    "            ##print('score:',score.shape)\n",
    "            # obtener predicción etiqueta\n",
    "            s_etiqueta = score_etiqueta[:,0]\n",
    "            etiqueta = torch.where(s_etiqueta >= thresholds['t_label'], 1, 0)\n",
    "            # obtener predicción domicilio\n",
    "            s_domicilio = score[:,1]\n",
    "            domicilio = torch.where(s_domicilio >= thresholds['t_num'], 1, 0)\n",
    "            # obtener predicción cara\n",
    "            s_cara = score[:,2]\n",
    "            s_no_cara = 1-s_cara\n",
    "            no_cara = torch.where(s_no_cara >= thresholds['t_face'], 1, 0)\n",
    "            \n",
    "            etiqueta = etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "            s_etiqueta = s_etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "            domicilio = domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "            s_domicilio = s_domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "            no_cara = no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "            s_no_cara = s_no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "            \n",
    "            #-----------------------------------------------------------------------------\n",
    "            # DETECTOR DE OBJETOS\n",
    "            od_prediction = od_model(image_od.to(device))\n",
    "            ##print(od_prediction)\n",
    "            dataFrame = pd.DataFrame(od_prediction).to_dict(orient=\"list\")\n",
    "            #print(dataFrame['boxes'])\n",
    "            #print(torch.Tensor(dataFrame['boxes']))\n",
    "            list_of_lists = list(map(lambda x: x.tolist(), dataFrame['boxes']))\n",
    "            array_of_arrays1= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['boxes'])))\n",
    "            array_of_arrays2= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['scores'])))\n",
    "            #bla = list(map(lambda x: torch.where(x >= 0.5, 1, 0), dataFrame['scores']))\n",
    "            bla2 = np.array(list(map(lambda x: np.where(x.to('cpu').numpy()>=thresholds['t_prod'],1,\n",
    "                                                        0).reshape(-1,1),\n",
    "                                     dataFrame['scores'])))\n",
    "            #print(array_of_arrays1)\n",
    "            #print(bla2)\n",
    "            producto = array_of_arrays1*bla2\n",
    "            #print(producto)\n",
    "            producto_wz = np.array(list(map(lambda x: x[~np.all(x == 0, axis=1)], producto)))\n",
    "            contexto = np.array(list(map(lambda x,y: bbox_function(x,y), producto_wz, area_img)))\n",
    "            contexto = contexto.reshape(-1,1)\n",
    "            ctx_value = np.where(((contexto >= thresholds['t_ctx_down']) & (contexto <= thresholds['t_ctx_up'])),\n",
    "                                 1, 0) \n",
    "            #print(ctx_value)\n",
    "            #print(array_of_arrays*array_of_arrays)\n",
    "            #print(array_of_arrays2)\n",
    "            #print(np.where(array_of_arrays2.all() < 0.5, 0, array_of_arrays2))\n",
    "            #print(list(map(lambda x: max(x), dataFrame['boxes'])))\n",
    "            s_paquete = torch.tensor(list(map(lambda x: max(x, default=0), dataFrame['scores'])))\n",
    "            s_paquete = s_paquete.reshape(-1,1)\n",
    "            #printtorch.tensor(dataFrame['scores'])\n",
    "            #print(scores.reshape(-1,1).to('cpu').numpy())\n",
    "            paquete = torch.where(s_paquete >= thresholds['t_prod'], 1, 0)\n",
    "            #############listos con score!!#################3\n",
    "            # que pasa si no se puede leer el url?\n",
    "            # que pasa si la imagen no tiene bbox? -> funciona todo bien\n",
    "            # falta enlace: corecto o incorrecto\n",
    "            \n",
    "            result = np.concatenate([paquete, s_paquete, etiqueta, s_etiqueta, no_cara, s_no_cara,\n",
    "                                    domicilio, s_domicilio, contexto, ctx_value], axis = 1)\n",
    "            \n",
    "            ##print(result)\n",
    "            df = pd.DataFrame(result, columns=['paquete','s_paquete','etiqueta_producto', \\\n",
    "                                                           's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\\\n",
    "                                                           'numero_domicilio', 's_numero_domicilio', 'contexto',\\\n",
    "                                                           'ctx_value'])\n",
    "            \n",
    "            pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2}\n",
    "            df['score'] = df.paquete*pesos['w_prod'] + df.etiqueta_producto*pesos['w_label'] + \\\n",
    "            df.sin_rostro*pesos['w_notface'] + df.numero_domicilio*pesos['w_num'] + df.ctx_value*pesos['w_contx']\n",
    "            #FALTA GUARDAR EL ENLACE Y NOTIFICAR CUANDO UN ENLACE ESTA ERRONEO\n",
    "            #df['url'] = img_url\n",
    "            df.insert(loc=0, column='url', value=img_url)\n",
    "            # concatenar a df anterior\n",
    "            df_base = pd.concat((df_base,df), ignore_index= True)\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.39 s (started: 2022-11-17 18:40:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_base = score(predictionLoader_mlc, predictionLoader_od, ml_model_1 , ml_model_2, \n",
    "          detector = od_model,pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, \n",
    "                                       'w_contx': 0.2},\n",
    "          thresholds = {'t_prod': 0.5, 't_face': 0.6491, 't_label': 0.3, 't_num': 0.6, 't_ctx_down': 0.2, \n",
    "                            't_ctx_up': 0.65}, device = 'cuda')\n",
    "t_fin = time.time()\n",
    "delta_time = t_fin-t_ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación nueva tabla BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.85 ms (started: 2022-11-17 18:42:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def table_generation(df_images, df_base, delta_time):\n",
    "    inner_merged = pd.merge(df_images, df_base, on=[\"url\"])\n",
    "    df_base_copy1 = inner_merged.copy()\n",
    "    # normalización\n",
    "    #agregar indice\n",
    "    largo_dataset = len(df_base_copy1)\n",
    "    df_base_copy1['index'] = df_base_copy1.index\n",
    "    # Cambiar tipo de dato de \"score\" a float\n",
    "    df_base_copy1[\"score\"] = pd.to_numeric(df_base_copy1[\"score\"])\n",
    "    # Seleccionar SOC que tiene mayor \"score\"\n",
    "    idx_max_score = df_base_copy1.groupby(['SOC'])['score'].transform(max) == df_base_copy1['score']\n",
    "    ##df_base_copy2 = df_base_copy1[idx_max_score].drop_duplicates(['SOC'])\n",
    "    # Agrupar por SOC y dejar indice\n",
    "    ##df_base_copy2 = df_base_copy1[['SOC', 'index', 'score']].groupby(['SOC'], sort = False).score.max()\n",
    "    # Agrupar por RUT, normalizar los nombres y dejar indice\n",
    "    #.agg(lambda x:x.value_counts().index[0])\n",
    "    df_base_copy3 = df_base_copy1[['provider_name', 'provider_id']].groupby(['provider_id'], sort = False)['provider_name'].transform('first').to_frame()\n",
    "    df_base_copy3['index'] = df_base_copy1['index']\n",
    "    # \"merge\" del dataset normalizado y el dataset original, con respecto al indice\n",
    "    df_base_copy4 = pd.merge(df_base_copy3, df_base_copy1, how='left', on='index')\n",
    "    # \"merge\" del dataset agrupado por SOC y el dataset anterior, con respecto al indice\n",
    "    ##df_base_final  = pd.concat([df_base_copy2, df_base_copy4], axis=1, join=\"inner\")\n",
    "    df_base_final = df_base_copy1[idx_max_score].drop_duplicates(['SOC'])\n",
    "    ##df_base_final = pd.merge(df_base_copy2, df_base_copy4, how='left', on='index')\n",
    "    # arreglar fechas\n",
    "    print(df_base_final['dfl_crte_tmst'])\n",
    "    df_base_final['dfl_crte_tmst'] = df_base_final['dfl_crte_tmst'].dt.tz_localize(None)\n",
    "    print(df_base_final['dfl_crte_tmst'])\n",
    "    df_base_final['event_crte_tmst'] = df_base_final['event_crte_tmst'].dt.tz_localize(None)\n",
    "    # guardar largo del dataset\n",
    "    new_df_base_final = df_base_final.assign(len_data = largo_dataset)\n",
    "    # guardar tiempo de ejecución\n",
    "    new_df_base_final = new_df_base_final.assign(execution_time_model = delta_time)\n",
    "    \n",
    "    return new_df_base_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2022-11-14 03:00:11.278000+00:00\n",
      "4   2022-11-14 03:00:20.229000+00:00\n",
      "9   2022-11-14 03:06:58.763000+00:00\n",
      "Name: dfl_crte_tmst, dtype: datetime64[ns, UTC]\n",
      "0   2022-11-14 03:00:11.278\n",
      "4   2022-11-14 03:00:20.229\n",
      "9   2022-11-14 03:06:58.763\n",
      "Name: dfl_crte_tmst, dtype: datetime64[ns]\n",
      "time: 27.8 ms (started: 2022-11-17 18:42:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "new_df_base_final = table_generation(df_images, df_base, delta_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 ms (started: 2022-11-16 20:30:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def tables_save(new_df_base_final, n_r, project_id = 'tc-sc-bi-bigdata-corp-tsod-dev', dataset = 'image_recognition', \n",
    "                full_table = 'prediction_table', random_table = 'random_table', \n",
    "                random_table_backup  = 'random_table_backup', prediction_if_exists = 'append',\n",
    "           random_if_exists = 'replace', random_backup_if_exists = 'append'):\n",
    "    # generar tabla aleatoria\n",
    "    df_copy = new_df_base_final.copy()\n",
    "    #df_filtrado = df_copy[(df_copy.enlace != 'incorrecto')]\n",
    "    df_copy=df_copy.assign(paquete_em=\"\", etiqueta_em=\"\", domicilio_em=\"\", rostro_em=\"\")\n",
    "    import pandas_gbq\n",
    "    # ubicación de destino para tabla procesada\n",
    "    destination_full_table = dataset + '.' + full_table\n",
    "    # anexar tabla de predicciones\n",
    "    pandas_gbq.to_gbq(new_df_base_final, destination_full_table, project_id=project_id, if_exists = prediction_if_exists)\n",
    "    print(\"Se almacenó exitosamente tabla de predicciones\")\n",
    "    # generar tabla aleatoria\n",
    "    try:\n",
    "        # seleccionar pequeño conjunto de datos aleatorios de cada sample\n",
    "        # intentar generar tabla aleatoria\n",
    "        random_sample = df_copy.sample(n_r, frac=None, replace=False, weights=None, random_state=None)\n",
    "        # destino de tabla aleatoria\n",
    "        destination_random_table = dataset + '.' + random_table\n",
    "        # anexar tabla aleatoria a tabla generada durante el dia\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table, project_id=project_id, \n",
    "                          if_exists = random_if_exists)\n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria\")\n",
    "        # anexar tabla aleatoria, con todas las tablas aleatorias generadas en dias anteriores\n",
    "        # para tenerlas de respaldo\n",
    "        destination_random_table_backup = dataset + '.' + random_table_backup\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table_backup, project_id=project_id, \n",
    "                          if_exists = random_backup_if_exists)  \n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria al backup\")\n",
    "    except:\n",
    "        print(\"No se pudo generar tabla aleatoria (muy pocas filas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12122.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente tabla de predicciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16008.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente sub-tabla aleatoria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15252.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente sub-tabla aleatoria al backup\n",
      "time: 17.3 s (started: 2022-11-16 20:30:05 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tables_save(new_df_base_final, n_r = 200, project_id = 'tc-sc-bi-bigdata-corp-tsod-dev', dataset = 'image_recognition', \n",
    "                full_table = 'prediction_table_optimizado_final', random_table = 'random_table_optimizado_final', \n",
    "                random_table_backup  = 'random_table_backup_optimizado_final',  prediction_if_exists = 'append',\n",
    "           random_if_exists = 'replace', random_backup_if_exists = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05887b1a958f4535938c3a9208b4ebaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9597f48902e54c7a9989484bcf6a239d",
      "placeholder": "​",
      "style": "IPY_MODEL_4a4aad02b8d24bffa799e6a5fdd4e006",
      "value": " 340M/340M [00:01&lt;00:00, 147MB/s]"
     }
    },
    "074323727f34416c895035b6d28e3738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a272d658182140b29fe755334af326c0",
      "placeholder": "​",
      "style": "IPY_MODEL_758e0c34b07e4c168617e8857e09b8fa",
      "value": " 160M/160M [00:01&lt;00:00, 90.2MB/s]"
     }
    },
    "1d7e20161e9248e6a41c541517122a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23c9ac6aff0448a2a69bf53e98c1b64a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32a0c7587cf74eeb98287fdf4767f938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e932cf1b4e484520a4dc4dd6672a7f17",
       "IPY_MODEL_e6dd759dedd84fe3afde7fafadbdc5e6",
       "IPY_MODEL_074323727f34416c895035b6d28e3738"
      ],
      "layout": "IPY_MODEL_43204103508c4482921d3f6bd45fe366"
     }
    },
    "3a331d5960ec443eb8064094c6f9a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b131ad046e94d0fa9405998f77117c1",
      "placeholder": "​",
      "style": "IPY_MODEL_719e9bde165b4db9816907ef904a8f67",
      "value": "100%"
     }
    },
    "3c468e9910384933988fb43f15ec7987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4271f826e55c4fee959a8097d9695e36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43204103508c4482921d3f6bd45fe366": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b4809efd1946ac911ccc2999bce65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a4aad02b8d24bffa799e6a5fdd4e006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bb2db3d2e644c26800558b8cdbc87fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a331d5960ec443eb8064094c6f9a414",
       "IPY_MODEL_a9e479cebc604561a5285a26a9b3cf9c",
       "IPY_MODEL_f94af2ae961249f1a2c6888009162afe"
      ],
      "layout": "IPY_MODEL_505388778fc745549e1be6b4478ada1b"
     }
    },
    "505388778fc745549e1be6b4478ada1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "719e9bde165b4db9816907ef904a8f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "758e0c34b07e4c168617e8857e09b8fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78b8a851199c421694811a3d2b09d77a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7b131ad046e94d0fa9405998f77117c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9597f48902e54c7a9989484bcf6a239d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97e08226970849feaea2b8c4a2c59162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fb6091980f34f1fa8b8a6168760a397": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c5367310094e45baa5e4f21daa819a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a23ac5b3ea2d4bd9950f901288642e51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a272d658182140b29fe755334af326c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4d602abedbf472ba8443b587b7172a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e369973d094144a583efe831a749932e",
       "IPY_MODEL_f185edec20214d4f9174944db636ce7b",
       "IPY_MODEL_05887b1a958f4535938c3a9208b4ebaf"
      ],
      "layout": "IPY_MODEL_a23ac5b3ea2d4bd9950f901288642e51"
     }
    },
    "a9e479cebc604561a5285a26a9b3cf9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fb6091980f34f1fa8b8a6168760a397",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b4809efd1946ac911ccc2999bce65d",
      "value": 167502836
     }
    },
    "c084ae1be8e94b6da09dde8e90362301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d482f72576e946f59c20fea52f2973c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e369973d094144a583efe831a749932e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d482f72576e946f59c20fea52f2973c3",
      "placeholder": "​",
      "style": "IPY_MODEL_a0c5367310094e45baa5e4f21daa819a",
      "value": "100%"
     }
    },
    "e6dd759dedd84fe3afde7fafadbdc5e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23c9ac6aff0448a2a69bf53e98c1b64a",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d7e20161e9248e6a41c541517122a17",
      "value": 167502836
     }
    },
    "e932cf1b4e484520a4dc4dd6672a7f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4271f826e55c4fee959a8097d9695e36",
      "placeholder": "​",
      "style": "IPY_MODEL_97e08226970849feaea2b8c4a2c59162",
      "value": "100%"
     }
    },
    "f185edec20214d4f9174944db636ce7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3c586b991004e839c47ee46163195e8",
      "max": 356082095,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78b8a851199c421694811a3d2b09d77a",
      "value": 356082095
     }
    },
    "f3c586b991004e839c47ee46163195e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f94af2ae961249f1a2c6888009162afe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c084ae1be8e94b6da09dde8e90362301",
      "placeholder": "​",
      "style": "IPY_MODEL_3c468e9910384933988fb43f15ec7987",
      "value": " 160M/160M [00:01&lt;00:00, 121MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaba844-9e5c-4e2e-818e-c064daa1cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-cloud-aiplatform --upgrade\n",
    "#!pip3 install kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de1694e-e037-4c73-b478-6475c83b0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones \n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01538b68-3871-4aba-9138-8e4ea1adfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"tc-sc-bi-bigdata-corp-tsod-dev\" \n",
    "BUCKET = \"gs://image-recognition-pipeline\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Pipeline root\n",
    "PIPELINE_ROOT = f\"{BUCKET}/PIPELINES/image-recognition/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a426feea-7576-449b-9881-85a9b6194bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT transport_ord_id as SOC, i.url as url, shipment.plate_num as plate_num, \n",
    "provider.doc_id as provider_id, \n",
    "provider.doc_verify_digit as provider_verify_digit,\n",
    "provider.name as provider_name, driver.doc_id as driver_id, \n",
    "driver.doc_verify_digit as driver_verify_digit,\n",
    "driver.name as driver_name, driver.last_name as driver_last_name,\n",
    "DATETIME(event_crte_tmst, 'America/Santiago') as event_crte_tmst, dfl_crte_tmst\n",
    "FROM \n",
    "`tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.btd_scha_fal_trmg_api_transport_order_temp`,\n",
    "unnest(image) as i\n",
    " \n",
    "WHERE\n",
    "  i.url is not null\n",
    "  and provider.name is not null\n",
    "  and provider.doc_id is not null\n",
    "  and DATE(event_crte_tmst, 'America/Santiago') = current_date() - 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c57a3-2aeb-41b7-8d33-4bb6e22e992d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c201297f-16d1-4b1a-9531-ee8f47308bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy tables\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\n",
    "                         \"db-dtypes\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"load_data.yaml\" \n",
    ")\n",
    "def copy_tables(\n",
    "    project_id:str,\n",
    "):  \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project = project_id)\n",
    "    \n",
    "    # TODO(developer): Set source_table_id to the ID of the original table.\n",
    "    source_table_id1 = \"tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.prediction_table_optimizado_final\"\n",
    "    source_table_id2 = \"tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.random_table_backup_optimizado_final\"\n",
    "    \n",
    "    # TODO(developer): Set destination_table_id to the ID of the destination table.\n",
    "    destination_table_id1 = \"tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.prediction_table_optimizado_final_copy\"\n",
    "    destination_table_id2 = \"tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.random_table_backup_optimizado_final_copy\"\n",
    "    \n",
    "    # Eliminar tablas copiadas\n",
    "    client.delete_table(destination_table_id1, not_found_ok=True)\n",
    "    client.delete_table(destination_table_id2, not_found_ok=True)\n",
    "    \n",
    "    # Copiar tablas originales\n",
    "    job = client.copy_table(source_table_id1, destination_table_id1)\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    print(\"A copy of the table created.\")\n",
    "    job = client.copy_table(source_table_id2, destination_table_id2)\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    print(\"A copy of the table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eff628e-d1a2-45ae-94b4-2545c78c435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\",\n",
    "                         \"google-cloud-bigquery\",\n",
    "                         \"db-dtypes\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"load_data.yaml\" \n",
    ")\n",
    "def load_data(\n",
    "    query:str,\n",
    "    project_id:str,\n",
    "    dataset:Output[Dataset],  \n",
    "):\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client(project = project_id)\n",
    "    df_base = client.query(query).to_dataframe().drop_duplicates(['url'])\n",
    "    \n",
    "    # Metadata\n",
    "    FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    dataset.metadata[\"updated_date\"] = FECHA\n",
    "    dataset.metadata[\"len_data\"] = len(df_base)\n",
    "    \n",
    "    # Save Artifact\n",
    "    df_base.to_csv(dataset.path + \".csv\", index = False, encoding = 'utf-8-sig')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99b47f5-74ba-4c9a-a832-df97bcd7d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to bucket\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\n",
    "                         \"google-cloud-storage\",\n",
    "                         \"numpy\",\n",
    "                         \"pandas\",\n",
    "                         \"opencv-python-headless\",\n",
    "                         \"db-dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"set_data_parallel.yaml\"\n",
    ")\n",
    "def set_data_parallel(\n",
    "    df_images:Input[Dataset],\n",
    "    project_id:str\n",
    ")-> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import cv2\n",
    "    import urllib\n",
    "    import urllib.request\n",
    "    from multiprocessing import cpu_count\n",
    "    from multiprocessing.pool import ThreadPool\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    def get_data(url):    \n",
    "        try:\n",
    "            url_str = url.split('/')[-1]\n",
    "            url_open = urllib.request.urlopen(url)\n",
    "            image_cv = np.asarray(bytearray(url_open.read()), dtype=\"uint8\")\n",
    "            image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            img_str = cv2.imencode('.png', image)[1].tostring()\n",
    "\n",
    "            storage_client = storage.Client()\n",
    "            bucket = storage_client.bucket('pod_images')\n",
    "            # el cero es para distinguir \n",
    "            blob = bucket.blob(f'{date.year}{date.month}{date.day}/{url_str}')\n",
    "            blob.upload_from_string(img_str)\n",
    "        except:\n",
    "            # ver el link que tira error\n",
    "            pass\n",
    "        \n",
    "    date = datetime.datetime.now()\n",
    "    client = bigquery.Client(project = project_id)\n",
    "    df_images = pd.read_csv(df_images.path + \".csv\")\n",
    "    # los ultimos 30 para ver si la maquina sirve\n",
    "    #urls = df_images.loc[:,'url'].iloc[:10]\n",
    "    urls = df_images.loc[:,'url']\n",
    "    \n",
    "    cpus = cpu_count()\n",
    "    results = ThreadPool(cpus-1).imap_unordered(get_data, urls)\n",
    "    # verificar que proceso termine\n",
    "    for _ in results:\n",
    "        pass\n",
    "    \n",
    "    # Salida\n",
    "    msg = \"start\"\n",
    "    return(msg,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404bde6c-89d2-46a6-8eab-09ab55b04eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob list\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\",\n",
    "                         \"db-dtypes\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"blob_name_list.yaml\" \n",
    ")\n",
    "def blob_name_list(\n",
    "    bucket_name:str,\n",
    "    output_path: OutputPath(),\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    #from tempfile import TemporaryFile\n",
    "    import datetime\n",
    "\n",
    "    date=datetime.datetime.now()\n",
    "\n",
    "    #nombre carpeta\n",
    "    #if date == \"today\":\n",
    "    #    folder = str(date.year) + str(date.month) + str(date.day)\n",
    "    #elif date == \"another day\":\n",
    "    #    folder = str(year) + str(month) + str(day)\n",
    "    #folder = str(2022) + str(12) + str(3) \n",
    "    folder = f'{date.year}{date.month}{date.day}'\n",
    "\n",
    "    # inicialización\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=folder)\n",
    "    # generar lista de nombres de blobs\n",
    "    name_blobs_list = list(blobs)\n",
    "    blobs_list = list(map(lambda x: x.name, name_blobs_list))\n",
    "    \n",
    "    import pickle\n",
    "\n",
    "    with open(output_path + \"blobs_list.pkl\", 'wb') as file:\n",
    "        pickle.dump(blobs_list, file)\n",
    "    \n",
    "    # Metadata\n",
    "    #FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #output_path.metadata[\"len_list\"] = len(blobs_list)\n",
    "    print(len(blobs_list))\n",
    "    \n",
    "    # Save Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656443ae-e85d-4d51-8b13-089e1eea612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open parallel bucket\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\",\n",
    "                         \"numpy\",\n",
    "                         \"opencv-python-headless\",\n",
    "                         \"db-dtypes\",\n",
    "                         \"psutil\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"open_parallel_bucket.yaml\" \n",
    ")\n",
    "def open_parallel_bucket(\n",
    "    input_path: InputPath(),\n",
    "    output_path: OutputPath()\n",
    ")-> NamedTuple(\"output\", [(\"time\", float)]):\n",
    "    import psutil\n",
    "\n",
    "    import pickle\n",
    "    file = open(input_path + \"blobs_list.pkl\", 'rb')\n",
    "    args = pickle.load(file)\n",
    "    \n",
    "    # importar tiempo\n",
    "    import time\n",
    "    t_ini = time.time()\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    # función para leer imágenes desde el bucket\n",
    "    def read_image_bucket(blob_name):\n",
    "        imagename = blob_name.split('/')[1]\n",
    "    \n",
    "        client = storage.Client()\n",
    "        \n",
    "        bucket = client.get_bucket('pod_images')\n",
    "        blob = bucket.get_blob(blob_name)\n",
    "        \n",
    "        try:\n",
    "            b = blob.download_as_bytes()\n",
    "            image_cv = np.asarray(bytearray(b), dtype=\"uint8\")\n",
    "            image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "            area = image.shape[0]*image.shape[1]\n",
    "            url = 'https://prdadessacorptrl.blob.core.windows.net/cl-images/' + imagename\n",
    "        except:\n",
    "            image = np.zeros((480,480,3), np.uint8)\n",
    "            area = image.shape[0]*image.shape[1]\n",
    "            url = 'https://prdadessacorptrl.blob.core.windows.net/cl-images/' + imagename\n",
    "            \n",
    "        return(image, area, url)\n",
    "    \n",
    "    from multiprocessing import cpu_count\n",
    "    from multiprocessing.pool import ThreadPool\n",
    "    \n",
    "    cpus = cpu_count()\n",
    "    #########################################################\n",
    "    print(cpus)\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/(1024.0 ** 3))\n",
    "    #########################################################\n",
    "    \n",
    "    results = ThreadPool(cpus).imap_unordered(read_image_bucket, args)\n",
    "    lista_base = []\n",
    "\n",
    "    for result in results:\n",
    "        lista_base.append(result) \n",
    "    \n",
    "    # calcular cuánto se demora en procesar\n",
    "    t_fin = time.time()\n",
    "    delta_time = t_fin-t_ini\n",
    "    \n",
    "    #########################################################\n",
    "    print(delta_time)\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/(1024.0 ** 3))\n",
    "    #########################################################\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open(output_path + \"image_bgr_list.pkl\", 'wb') as file:\n",
    "        pickle.dump(lista_base, file)\n",
    "    \n",
    "    # Metadata\n",
    "    #FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #dataset.metadata[\"updated_date\"] = FECHA\n",
    "    \n",
    "    # Save Artifact\n",
    "    #new_df_base_final.to_csv(dataset.path + \".csv\", index = False, encoding = 'utf-8-sig')\n",
    "    return(delta_time,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f10ebf-94dc-423e-9172-5b532b4b45a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\",\n",
    "                         \"numpy\",\n",
    "                         \"google-cloud-bigquery\",\n",
    "                         \"google-cloud-storage\",\n",
    "                         \"db-dtypes\",\n",
    "                         \"scikit-image\",\n",
    "                         \"psutil\",\n",
    "                         \"opencv-python-headless\"],\n",
    "    base_image = \"gcr.io/deeplearning-platform-release/pytorch-gpu\",\n",
    "    output_component_file=\"prediction.yaml\" \n",
    ")\n",
    "def prediction(\n",
    "    project_id:str,\n",
    "    input_path:InputPath(),\n",
    "    dataset:Output[Dataset] \n",
    ")-> NamedTuple(\"output\", [(\"time\", float)]):\n",
    "    import psutil\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    file = open(input_path + \"blobs_list.pkl\", 'rb')\n",
    "    blobs_list = pickle.load(file)\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import torch\n",
    "    import torchvision\n",
    "    #from torchvision.models import resnext101_32x8d\n",
    "    from torchvision.models import resnext101_32x8d, ResNeXt101_32X8D_Weights\n",
    "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    #########################################################\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/(1024.0 ** 3))\n",
    "    #########################################################\n",
    "\n",
    "    # Definir resnext\n",
    "    #resnext1 = resnext101_32x8d(pretrained=True)\n",
    "    #resnext2 = resnext101_32x8d(pretrained=True)\n",
    "    weights = ResNeXt101_32X8D_Weights.DEFAULT\n",
    "    resnext1 = resnext101_32x8d(weights)\n",
    "    resnext2 = resnext101_32x8d(weights)\n",
    "    \n",
    "    # Obtener clases para modelos\n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name, project_id):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        storage_client = storage.Client(project= project_id)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        print(\n",
    "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                source_blob_name, bucket_name, destination_file_name\n",
    "            )\n",
    "        )\n",
    "    bucket = \"librerias_pod_images\" \n",
    "    download_blob(bucket, \"models_import2.py\", \"models_import2.py\", project_id)\n",
    "    \n",
    "    from models_import2 import MultilabelClassifier1, MultilabelClassifier2\n",
    "    \n",
    "    # cargar modelo para cara y n° domicilio\n",
    "    download_blob(bucket, \"Models/mlc_model_4.pth\", \"mlc_model_4.pth\", project_id)\n",
    "    device = 'cuda'\n",
    "    PATH_1 = 'mlc_model_4.pth'\n",
    "    ml_model_1 = MultilabelClassifier1(3, resnext1).to(device)\n",
    "    ml_model_dict_1 = torch.load(PATH_1, map_location=torch.device(device))\n",
    "    ml_model_1.load_state_dict(ml_model_dict_1['model_state_dict'])\n",
    "    \n",
    "    # cargar modelo para etiqueta del producto\n",
    "    download_blob(bucket, \n",
    "                  \"Models/mlc_model_baseline_data_revisada_1_2.pth\", \n",
    "                  \"mlc_model_baseline_data_revisada_1_2.pth\", project_id)\n",
    "    device = 'cuda'\n",
    "    PATH_2 = 'mlc_model_baseline_data_revisada_1_2.pth'\n",
    "    ml_model_2 = MultilabelClassifier2(3, resnext2).to(device)\n",
    "    ml_model_dict_2 = torch.load(PATH_2, map_location=torch.device(device))\n",
    "    ml_model_2.load_state_dict(ml_model_dict_2['model_state_dict'])\n",
    "    \n",
    "    def get_object_detection_model(num_classes):\n",
    "        # load a model pre-trained on COCO\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "        return model\n",
    "    \n",
    "    # cargar modelo detector de objetos\n",
    "    download_blob(bucket, \n",
    "                  \"Models/model_cf_12y3_1.pth\", \n",
    "                  \"model_cf_12y3_1.pth\", project_id)\n",
    "    device = 'cuda'\n",
    "    model_path = 'model_cf_12y3_1.pth'\n",
    "    num_classes = 2\n",
    "    od_model = get_object_detection_model(num_classes)\n",
    "    od_model_dict = torch.load(model_path, map_location=torch.device(device))\n",
    "    od_model.load_state_dict(od_model_dict['model_state_dict'])\n",
    "    \n",
    "    #########################################################\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/(1024.0 ** 3))\n",
    "    #########################################################\n",
    "    \n",
    "    from models_import2 import PredictionDataset\n",
    "    \n",
    "    # se generan batch para clasificador multi-etiqueta y detector de objetos\n",
    "    prediction_set_mlc = PredictionDataset(bucket_name = 'pod_images', blob_name_list = blobs_list,\n",
    "                                           normalization = 'mlc', device = 'cuda')\n",
    "    prediction_set_od = PredictionDataset(bucket_name = 'pod_images', blob_name_list = blobs_list,\n",
    "                                          normalization = 'od', device = 'cuda')\n",
    "\n",
    "    # para aumentar más el batch_size hay que agregar más memoria\n",
    "    predictionLoader_mlc = DataLoader(prediction_set_mlc, batch_size=32, num_workers=2,\n",
    "                                      shuffle = False, pin_memory = True)\n",
    "    predictionLoader_od = DataLoader(prediction_set_od, batch_size=32, num_workers=2,\n",
    "                                     shuffle = False, pin_memory = True)\n",
    "    \n",
    "    import itertools\n",
    "    \n",
    "    def bbox_function(b, area_img):\n",
    "        l =list(range(b.shape[0]))\n",
    "        # recorrer cajas y calcular inter-area para cada combinación\n",
    "        indice = 0\n",
    "        interArea = 0\n",
    "        for i in itertools.combinations(l, r=2):\n",
    "            # obtener coordenadas de la inter-area\n",
    "            x0 = max(b[i[0]][0], b[i[1]][0])\n",
    "            y0 = max(b[i[0]][1], b[i[1]][1])\n",
    "            x1 = min(b[i[0]][2], b[i[1]][2])\n",
    "            y1 = min(b[i[0]][3], b[i[1]][3])\n",
    "    \n",
    "            # calcular inter-area\n",
    "            dif_x = x0-x1\n",
    "            dif_y = y0-y1\n",
    "            # se verifica que las esquinas de la interArea esten bien ubicadas \n",
    "            if dif_x < 0 and dif_y < 0:\n",
    "                interArea += dif_x*dif_y\n",
    "                ##interArea += abs(x0-x1)*abs(y0-y1)\n",
    "        \n",
    "        # sumar areas de cada bbox\n",
    "        area_total_bbox = 0\n",
    "        for box in b:\n",
    "            # calcular area de cada bbox\n",
    "            area_bbox = abs(box[0]-box[2])*abs(box[1]-box[3])\n",
    "            # calcular area total de bbox\n",
    "            area_total_bbox += area_bbox\n",
    "    \n",
    "        # calcular la union de las areas de cada bbox\n",
    "        union = area_total_bbox - interArea\n",
    "    \n",
    "        # calcular contexto\n",
    "        if torch.is_tensor(union):\n",
    "            contexto = union.item()/area_img\n",
    "        \n",
    "        else:\n",
    "            contexto = union/area_img\n",
    "        \n",
    "        return contexto\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    def score(predictionLoader_mlc, predictionLoader_od, classificator_1, classificator_2, \n",
    "              detector = od_model, pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, \n",
    "                                            'w_contx': 0.2},\n",
    "              thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "                            't_ctx_up': 0.65}, device = 'cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classificator_1 (modelo): clasificador multi-etiqueta para detectar la etiqueta del paquete\n",
    "            classificator_2 (modelo): clasificador multi-etiqueta para detectar la cara y el domicilio\n",
    "            detector (modelo): detector de objetos para identificar el paquete y su bbox respectivo\n",
    "            pesos (dict): diccionario con valores para cada peso, es decir,\n",
    "            w_prod (product), w_notface (without face), w_label (product label), \n",
    "            w_num (address number) y w_contx (context)\n",
    "            thresholds = diccionario con valores de umbral para cada criterio, es decir,\n",
    "            t_prod (product), t_face (face detector), t_label (product label) y \n",
    "            t_num (address number)\n",
    "            \n",
    "        Obs: los pesos deben sumar 1 para todos los criterios menos el del numero de domicilio. Este\n",
    "            ultimo corresponde a un beneficio de +0.1 si es que aparece en la fotografía.\n",
    "    \n",
    "        Returns:\n",
    "            result_data (dataFrame): cada una de las columnas del dataFrame corresponde a la predicción\n",
    "            de cada criterio sobre cierto umbral, los scores (confianza del modelo) de cada criterio y \n",
    "            la nota de la foto (score).\n",
    "        \"\"\"\n",
    "        # evaluar con gpu o cpu \n",
    "        device = torch.device(device)\n",
    "        \n",
    "        classificator_1.to(device)\n",
    "        classificator_1.eval()  \n",
    "        classificator_2.to(device)\n",
    "        classificator_2.eval()           \n",
    "        detector.to(device)\n",
    "        detector.eval()\n",
    "        df_base = pd.DataFrame(columns = ['url','paquete', 's_paquete', 'etiqueta_producto',\n",
    "                                              's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\n",
    "                                              'numero_domicilio', 's_numero_domicilio', 'contexto',\n",
    "                                              'ctx_value', 'score'])\n",
    "        with torch.no_grad():\n",
    "            for i, image in enumerate(zip(predictionLoader_mlc, predictionLoader_od)):\n",
    "                # se obtiene la imagen de cada dataLoader\n",
    "                image_mlc = image[0][0]\n",
    "                image_od = image[1][0]\n",
    "                \n",
    "                # obtener areas y urls de las imagenes\n",
    "                area_img = image[0][1]\n",
    "                #img_url = np.asarray(image[0][2]).reshape(-1,1)\n",
    "                img_url = list(image[0][2])\n",
    "        \n",
    "                # CLASIFICACION MULTI-ETIQUETA\n",
    "                #---------------------------clasificador 1--------------------------------\n",
    "                label_base = classificator_1(image_mlc.to(device))\n",
    "                #---------------------------clasificador 2--------------------------------\n",
    "                label_etiqueta = classificator_2(image_mlc.to(device))\n",
    "                # obtener score\n",
    "                score_etiqueta = torch.sigmoid(label_etiqueta['label']).squeeze()\n",
    "                score = torch.sigmoid(label_base['label']).squeeze()\n",
    "                # condición para cuando se tiene un batch de 1 elemento\n",
    "                if score_etiqueta.dim() == 1:\n",
    "                    score_etiqueta = torch.unsqueeze(score_etiqueta, dim=0)\n",
    "                    score = torch.unsqueeze(score, dim=0)\n",
    "                # obtener predicción etiqueta\n",
    "                s_etiqueta = score_etiqueta[:,0]\n",
    "                etiqueta = torch.where(s_etiqueta >= thresholds['t_label'], 1, 0)\n",
    "                # obtener predicción domicilio\n",
    "                s_domicilio = score[:,1]\n",
    "                domicilio = torch.where(s_domicilio >= thresholds['t_num'], 1, 0)\n",
    "                # obtener predicción cara\n",
    "                s_cara = score[:,2]\n",
    "                s_no_cara = 1-s_cara\n",
    "                no_cara = torch.where(s_no_cara >= thresholds['t_face'], 1, 0)\n",
    "                \n",
    "                etiqueta = etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "                s_etiqueta = s_etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "                domicilio = domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "                s_domicilio = s_domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "                no_cara = no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "                s_no_cara = s_no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "                \n",
    "                #-----------------------------------------------------------------------------\n",
    "                # DETECTOR DE OBJETOS\n",
    "                od_prediction = od_model(image_od.to(device))\n",
    "                dataFrame = pd.DataFrame(od_prediction).to_dict(orient=\"list\")\n",
    "                list_of_lists = list(map(lambda x: x.tolist(), dataFrame['boxes']))\n",
    "                array_of_arrays1= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['boxes'])))\n",
    "                array_of_arrays2= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['scores'])))\n",
    "                #bla = list(map(lambda x: torch.where(x >= 0.5, 1, 0), dataFrame['scores']))\n",
    "                bla2 = np.array(list(map(lambda x: np.where(x.to('cpu').numpy()>=thresholds['t_prod'],1,\n",
    "                                                            0).reshape(-1,1),\n",
    "                                         dataFrame['scores'])))\n",
    "                producto = array_of_arrays1*bla2\n",
    "                producto_wz = np.array(list(map(lambda x: x[~np.all(x == 0, axis=1)], producto)))\n",
    "                contexto = np.array(list(map(lambda x,y: bbox_function(x,y), producto_wz, area_img)))\n",
    "                contexto = contexto.reshape(-1,1)\n",
    "                ctx_value = np.where(((contexto >= thresholds['t_ctx_down']) & (contexto <= thresholds['t_ctx_up'])),\n",
    "                                     1, 0) \n",
    "                s_paquete = torch.tensor(list(map(lambda x: max(x, default=0), dataFrame['scores'])))\n",
    "                s_paquete = s_paquete.reshape(-1,1)\n",
    "                paquete = torch.where(s_paquete >= thresholds['t_prod'], 1, 0)\n",
    "                # Obtener array de resultados\n",
    "                result = np.concatenate([paquete, s_paquete, etiqueta, s_etiqueta, no_cara, s_no_cara,\n",
    "                                        domicilio, s_domicilio, contexto, ctx_value], axis = 1)\n",
    "                \n",
    "                # LLevarlo a un dataFrame\n",
    "                df = pd.DataFrame(result, columns=['paquete','s_paquete','etiqueta_producto', \\\n",
    "                                                               's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\\\n",
    "                                                               'numero_domicilio', 's_numero_domicilio', 'contexto',\\\n",
    "                                                               'ctx_value'])\n",
    "                \n",
    "                pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2}\n",
    "                df['score'] = df.paquete*pesos['w_prod'] + df.etiqueta_producto*pesos['w_label'] + \\\n",
    "                df.sin_rostro*pesos['w_notface'] + df.numero_domicilio*pesos['w_num'] + df.ctx_value*pesos['w_contx']\n",
    "                df.insert(loc=0, column='url', value=img_url)\n",
    "                # concatenar a df anterior\n",
    "                df_base = pd.concat((df_base,df), ignore_index= True)\n",
    "        return df_base\n",
    "\n",
    "    # importar tiempo\n",
    "    import time\n",
    "    t_ini = time.time()\n",
    "    \n",
    "    df_prediction = score(predictionLoader_mlc, predictionLoader_od, ml_model_1 , ml_model_2, \n",
    "                          detector = od_model,pesos = {'w_prod': 0.4, 'w_notface': 0.1,\n",
    "                                                       'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2},\n",
    "                          thresholds = {'t_prod': 0.5, 't_face': 0.6491, 't_label': 0.3, 't_num': 0.6, \n",
    "                                        't_ctx_down': 0.2, 't_ctx_up': 0.65}, device = 'cuda')\n",
    "    \n",
    "    # calcular cuánto se demora en procesar\n",
    "    t_fin = time.time()\n",
    "    delta_time = t_fin-t_ini\n",
    "    \n",
    "    #########################################################\n",
    "    print(delta_time)\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/(1024.0 ** 3))\n",
    "    #########################################################\n",
    "    \n",
    "    # Metadata\n",
    "    import datetime\n",
    "    FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    dataset.metadata[\"updated_date\"] = FECHA\n",
    "    \n",
    "    # Save Artifact\n",
    "    df_prediction.to_csv(dataset.path + \".csv\", index = False, encoding = 'utf-8-sig')\n",
    "    \n",
    "    return(delta_time,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e24bb4-90c0-48f8-852e-3e79908997f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table generator\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"table_generation.yaml\" \n",
    ")\n",
    "def table_generation(\n",
    "    delta_time1: float,\n",
    "    df_images:Input[Dataset],\n",
    "    df_base:Input[Dataset],\n",
    "    dataset:Output[Dataset],  \n",
    "):\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    \n",
    "    delta_time = delta_time1\n",
    "    \n",
    "    df_images = pd.read_csv(df_images.path + \".csv\")\n",
    "    df_base = pd.read_csv(df_base.path + \".csv\") # prediction\n",
    "    \n",
    "    inner_merged = pd.merge(df_images, df_base, on=[\"url\"])\n",
    "    df_base_copy1 = inner_merged.copy()\n",
    "    # normalización\n",
    "    #agregar indice\n",
    "    largo_dataset = len(df_base_copy1)\n",
    "    df_base_copy1['index'] = df_base_copy1.index\n",
    "    # Cambiar tipo de dato de \"score\" a float\n",
    "    df_base_copy1[\"score\"] = pd.to_numeric(df_base_copy1[\"score\"])\n",
    "    # Seleccionar SOC que tiene mayor \"score\"\n",
    "    idx_max_score = df_base_copy1.groupby(['SOC'])['score'].transform(max) == df_base_copy1['score']\n",
    "    # Agrupar por RUT, normalizar los nombres y dejar indice\n",
    "    #df_base_copy3 = df_base_copy1[['provider_name', 'provider_id']].groupby(['provider_id'], sort = False)['provider_name'].transform('first').to_frame() # primer valor\n",
    "    df_base_copy1.provider_name = df_base_copy1.provider_name.str.strip() # quitar espacios al inicio y final\n",
    "    df_base_copy1.provider_name = df_base_copy1.provider_name.str.upper() # dejar todo en mayuscula\n",
    "    df_base_copy3 = df_base_copy1[['provider_name', 'provider_id']].groupby(['provider_id'], sort = False)['provider_name'].transform(lambda x: pd.Series.mode(x)[0]).to_frame() # valor más frecuente\n",
    "    df_base_copy3['index'] = df_base_copy1['index']\n",
    "    # \"merge\" del dataset normalizado y el dataset original, con respecto al indice\n",
    "    df_base_copy4 = pd.merge(df_base_copy3, df_base_copy1, how='left', on='index')\n",
    "    df_base_copy4.provider_name_y = df_base_copy4.provider_name_x\n",
    "    df_base_copy4 = df_base_copy4.drop(columns=[\"provider_name_x\"])\n",
    "    df_base_copy4.rename(columns = {'provider_name_y':'provider_name'}, inplace = True)\n",
    "    # \"merge\" del dataset agrupado por SOC y el dataset anterior, con respecto al indice\n",
    "    df_base_final = df_base_copy1[idx_max_score].drop_duplicates(['SOC'])\n",
    "    ##df_base_final = df_base_copy4[idx_max_score].drop_duplicates(['SOC'])\n",
    "    # arreglar fechas\n",
    "    ##df_base_final['dfl_crte_tmst'] = df_base_final['dfl_crte_tmst'].dt.tz_localize(None)\n",
    "    ##df_base_final['event_crte_tmst'] = df_base_final['event_crte_tmst'].dt.tz_localize(None)\n",
    "    #df_base_final['dfl_crte_tmst'] = pd.to_datetime(df_base_final['dfl_crte_tmst'], utc=True).dt.tz_localize(None)\n",
    "    #df_base_final['event_crte_tmst'] = pd.to_datetime(df_base_final['event_crte_tmst'], utc=True).dt.tz_localize(None)\n",
    "    # guardar largo del dataset\n",
    "    new_df_base_final = df_base_final.assign(len_data = largo_dataset)\n",
    "    # guardar tiempo de ejecución\n",
    "    new_df_base_final = new_df_base_final.assign(execution_time_model = delta_time)\n",
    "    \n",
    "    # Metadata\n",
    "    FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    dataset.metadata[\"updated_date\"] = FECHA\n",
    "    \n",
    "    # Save Artifact\n",
    "    new_df_base_final.to_csv(dataset.path + \".csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5024c2a8-aacb-42f1-af75-2f112df9f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tables\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\",\n",
    "                         \"pandas-gbq\",\n",
    "                         \"google-cloud-bigquery\",\n",
    "                         \"db-dtypes\"],\n",
    "    base_image = \"python:3.9\",\n",
    "    output_component_file=\"save_tables.yaml\" \n",
    ")\n",
    "def save_tables(\n",
    "    new_df_base_final:Input[Dataset],\n",
    "    n_r:int,\n",
    "    project_id:str,\n",
    "    dataset:str,\n",
    "    full_table:str, \n",
    "    random_table:str, \n",
    "    random_table_backup:str,  \n",
    "    prediction_if_exists:str,\n",
    "    random_if_exists:str, \n",
    "    random_backup_if_exists:str,\n",
    "    output_dataset: Output[Dataset],  \n",
    "):\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    \n",
    "    new_df_base_final = pd.read_csv(new_df_base_final.path + \".csv\")\n",
    "    \n",
    "    # cambiar formato de columnas\n",
    "    # int to string\n",
    "    new_df_base_final['SOC'] = new_df_base_final['SOC'].astype(str)\n",
    "    # string to timestamp\n",
    "    new_df_base_final['dfl_crte_tmst'] = pd.to_datetime(new_df_base_final['dfl_crte_tmst'], utc=True).dt.tz_localize(None)\n",
    "    new_df_base_final['event_crte_tmst'] = pd.to_datetime(new_df_base_final['event_crte_tmst'], utc=True).dt.tz_localize(None)\n",
    "    \n",
    "    # generar tabla aleatoria\n",
    "    df_copy = new_df_base_final.copy()\n",
    "    #df_filtrado = df_copy[(df_copy.enlace != 'incorrecto')]\n",
    "    df_copy=df_copy.assign(paquete_em=\"\", etiqueta_em=\"\", domicilio_em=\"\", rostro_em=\"\")\n",
    "    import pandas_gbq\n",
    "    # ubicación de destino para tabla procesada\n",
    "    destination_full_table = dataset + '.' + full_table\n",
    "    # anexar tabla de predicciones\n",
    "    pandas_gbq.to_gbq(new_df_base_final, destination_full_table, project_id=project_id, if_exists = prediction_if_exists)\n",
    "    #print(\"Se almacenó exitosamente tabla de predicciones\")\n",
    "    # generar tabla aleatoria\n",
    "    try:\n",
    "        # seleccionar pequeño conjunto de datos aleatorios de cada sample\n",
    "        # intentar generar tabla aleatoria\n",
    "        random_sample = df_copy.sample(n_r, frac=None, replace=False, weights=None, random_state=None)\n",
    "        # destino de tabla aleatoria\n",
    "        destination_random_table = dataset + '.' + random_table\n",
    "        # anexar tabla aleatoria a tabla generada durante el dia\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table, project_id=project_id, \n",
    "                          if_exists = random_if_exists)\n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria\")\n",
    "        # anexar tabla aleatoria, con todas las tablas aleatorias generadas en dias anteriores\n",
    "        # para tenerlas de respaldo\n",
    "        destination_random_table_backup = dataset + '.' + random_table_backup\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table_backup, project_id=project_id, \n",
    "                          if_exists = random_backup_if_exists)  \n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria al backup\")\n",
    "    except:\n",
    "        print(\"No se pudo generar tabla aleatoria (muy pocas filas)\")\n",
    "    \n",
    "    # Metadata\n",
    "    FECHA = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    output_dataset.metadata[\"updated_date\"] = FECHA\n",
    "    \n",
    "    # Save Artifact\n",
    "    new_df_base_final.to_csv(output_dataset.path + \".csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b94bf7-33d4-4e43-94ea-fa62f37ce5a8",
   "metadata": {},
   "source": [
    "# Definir Máquinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb80deb0-791b-4728-8300-33403260a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "# Convert the above component into a custom training job\n",
    "ctj_set_data = create_custom_training_job_from_component(\n",
    "    set_data_parallel,\n",
    "    display_name = 'set_data_parallel',\n",
    "    machine_type = 'n1-highcpu-16'\n",
    ")\n",
    "\n",
    "ctj_image_bgr_list = create_custom_training_job_from_component(\n",
    "    open_parallel_bucket,\n",
    "    display_name = 'open_parallel_bucket',\n",
    "    machine_type = 'n1-highmem-16'\n",
    ")\n",
    "\n",
    "ctj_prediction_op = create_custom_training_job_from_component(\n",
    "    prediction,\n",
    "    display_name = 'open_parallel_bucket',\n",
    "    machine_type = 'n1-highmem-16',\n",
    "    accelerator_type='NVIDIA_TESLA_T4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10988e-970d-4dbc-8d70-761f060abb94",
   "metadata": {},
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdadf077-6ddb-47e5-907c-799d53e00478",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    name = \"pipeline-image-recognition\",\n",
    ")\n",
    "def pipeline(\n",
    "    query:str = query,\n",
    "    project_id:str = PROJECT_ID,\n",
    "    bucket_name:str = 'pod_images',\n",
    "    n_r:int = 200,\n",
    "    dataset:str = 'image_recognition',\n",
    "    full_table:str = 'prediction_table_optimizado_final', \n",
    "    random_table:str = 'random_table_optimizado_final', \n",
    "    random_table_backup:str  = 'random_table_backup_optimizado_final',  \n",
    "    prediction_if_exists:str = 'append',\n",
    "    random_if_exists:str = 'replace', \n",
    "    random_backup_if_exists:str = 'append'\n",
    "):\n",
    "    # crear backup\n",
    "    backup = copy_tables(project_id)\n",
    "    # obtener data\n",
    "    data_op = load_data(query, project_id)\n",
    "    # cargar imagenes al bucket\n",
    "    set_data_parallel(data_op.output, project_id)\n",
    "    set_data = ctj_set_data(project=project_id).set_args(data_op.output, project_id).set_display_name('ctj_set_data')\n",
    "    ##set_data = set_data_parallel(data_op.output, project_id).set_cpu_limit('16').set_memory_limit('14G') #n1-highcpu-16 $0.453 hourly\n",
    "    # una vez se terminan de cargar los datos, comienza la etapa de extracción\n",
    "    with dsl.Condition(\n",
    "        set_data.outputs[\"deploy\"] == \"start\",\n",
    "        name = \"leer-imagenes\",\n",
    "    ):\n",
    "        # obtener lista de nombres\n",
    "        blob_list = blob_name_list(bucket_name)\n",
    "        # obtener lista de imagenes\n",
    "        image_bgr_list = ctj_image_bgr_list(project=project_id,\n",
    "                                           ).set_display_name('ctj_image_bgr_list').inputs(blob_list.output)\n",
    "        ##image_bgr_list = open_parallel_bucket(blob_list.output).set_cpu_limit('16').set_memory_limit('104G') #n1-highmem-16 $0.719 hourly\n",
    "        # image_bgr_list.output\n",
    "        # obtener DataFrame de predicciones\n",
    "        #n1-standard-8 + 1 NVIDIA T4\n",
    "        prediction_op = ctj_prediction_op(project=project_id,\n",
    "                                           ).set_display_name('ctj_prediction_op').inputs(project_id, image_bgr_list.outputs[\"output_path\"])\n",
    "        ##prediction_op = prediction(project_id, image_bgr_list.outputs[\"output_path\"]).set_cpu_limit('8').set_memory_limit('104G').add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "        #prediction_op.output\n",
    "        # obtener tablas: prediction, random y random backup\n",
    "        df_final = table_generation(image_bgr_list.outputs[\"time\"], prediction_op.outputs[\"time\"], \n",
    "                                    data_op.output, prediction_op.outputs[\"dataset\"])\n",
    "        # guardar tablas en BigQuery\n",
    "        save_tables(df_final.output, n_r, project_id, dataset, full_table, random_table, \n",
    "                    random_table_backup, prediction_if_exists, random_if_exists, \n",
    "                    random_backup_if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06612157-ed55-431d-89de-ebc8b8ba94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    name = \"pipeline-image-recognition2\",\n",
    ")\n",
    "def pipeline(\n",
    "    query:str = query,\n",
    "    project_id:str = PROJECT_ID,\n",
    "    bucket_name:str = 'pod_images',\n",
    "    n_r:int = 1000,\n",
    "    dataset:str = 'image_recognition',\n",
    "    full_table:str = 'prediction_table_optimizado_final', \n",
    "    random_table:str = 'random_table_optimizado_final', \n",
    "    random_table_backup:str  = 'random_table_backup_optimizado_final',  \n",
    "    prediction_if_exists:str = 'append',\n",
    "    random_if_exists:str = 'replace', \n",
    "    random_backup_if_exists:str = 'append'\n",
    "):\n",
    "    # crear backup\n",
    "    backup = copy_tables(project_id)\n",
    "    # obtener data\n",
    "    data_op = load_data(query, project_id)\n",
    "    # cargar imagenes al bucket\n",
    "    set_data = set_data_parallel(data_op.output, project_id).set_cpu_limit('32').set_memory_limit('15G') \n",
    "    #e2-highcpu-16\n",
    "    # una vez se terminan de cargar los datos, comienza la etapa de extracción\n",
    "    with dsl.Condition(\n",
    "        set_data.outputs[\"deploy\"] == \"start\",\n",
    "        name = \"leer-imagenes\",\n",
    "    ):\n",
    "        # obtener lista de nombres\n",
    "        blob_list = blob_name_list(bucket_name)\n",
    "        # obtener DataFrame de predicciones\n",
    "        prediction_op = prediction(project_id, blob_list.output).set_cpu_limit('4').set_memory_limit('15G').add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "        #n1-standard-8 + 1 NVIDIA T4\n",
    "        # obtener tablas: prediction, random y random backup\n",
    "        df_final = table_generation(prediction_op.outputs[\"time\"], \n",
    "                                    data_op.output, prediction_op.outputs[\"dataset\"])\n",
    "        # guardar tablas en BigQuery\n",
    "        save_tables(df_final.output, n_r, project_id, dataset, full_table, random_table, \n",
    "                    random_table_backup, prediction_if_exists, random_if_exists, \n",
    "                    random_backup_if_exists)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93c5f3e-a7d6-4ce5-8302-da4e127d812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    name = \"pipeline-image-recognition2\",\n",
    ")\n",
    "def pipeline(\n",
    "    query:str = query,\n",
    "    project_id:str = PROJECT_ID,\n",
    "    bucket_name:str = 'pod_images',\n",
    "    n_r:int = 1000,\n",
    "    dataset:str = 'image_recognition',\n",
    "    full_table:str = 'prediction_table_optimizado_final', \n",
    "    random_table:str = 'random_table_optimizado_final', \n",
    "    random_table_backup:str  = 'random_table_backup_optimizado_final',  \n",
    "    prediction_if_exists:str = 'append',\n",
    "    random_if_exists:str = 'replace', \n",
    "    random_backup_if_exists:str = 'append'\n",
    "):\n",
    "\n",
    "    # obtener data\n",
    "    data_op = load_data(query, project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0880ea61-41cd-44ea-81a0-4b6768f0f6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path='pipeline_ImageRecognition2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bd3ccb7-894d-43f3-bc97-4734a1197491",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name = 'pipeline-image-recognition2',\n",
    "    template_path = 'pipeline_ImageRecognition2.json',\n",
    "    enable_caching = False, # poner False cuando se pase a producción\n",
    "    location = REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc24738e-89d5-4309-bd9b-50be90ad11da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1003479373544/locations/us-central1/pipelineJobs/pipeline-image-recognition2-20230117211227\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1003479373544/locations/us-central1/pipelineJobs/pipeline-image-recognition2-20230117211227')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-image-recognition2-20230117211227?project=1003479373544\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c21e9d-e9a3-4306-9341-0d7d1f1f79e1",
   "metadata": {},
   "source": [
    "# Pipeline lectura por batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b91ace-d1d5-47bc-85e2-bf1c25837737",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (1048305445.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_20047/1048305445.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    query:str,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "@kfl.dsl.pipeline(\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    name = 'pipeline-ImageReconition',\n",
    ")\n",
    "def pipeline(\n",
    "    query:str,\n",
    "    project_id:str = PROJECT_ID,\n",
    "    date:str,\n",
    "    bucket_name:str = 'pod_images',\n",
    "    year:int,\n",
    "    month:int,\n",
    "    day:int,\n",
    "    n_p:int = 10,\n",
    "    n_r:int = 1,\n",
    "    dataset:str = 'image_recognition',\n",
    "    full_table:str = 'prediction_table_optimizado_final_pipeline', \n",
    "    random_table:str = 'random_table_optimizado_final_pipeline', \n",
    "    random_table_backup:str  = 'random_table_backup_optimizado_final_pipeline',  \n",
    "    prediction_if_exists:str = 'append',\n",
    "    random_if_exists:str = 'replace', \n",
    "    random_backup_if_exists:str = 'append'\n",
    "):\n",
    "    data_op = load_data(query, project_id)\n",
    "    blob_list = blob_name_list(date, bucket_name, year, month, day)\n",
    "    # particionar lista de nombres\n",
    "    partitions = [blob_list.output[i:i + n_p] for i in range(0, len(blob_list.output[0:100]), n_p)]\n",
    "    # df_base\n",
    "    #df_prediction = pd.DataFrame(columns = ['url','paquete', 's_paquete', 'etiqueta_producto',\n",
    "    #                                      's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\n",
    "    #                                      'numero_domicilio', 's_numero_domicilio', 'contexto',\n",
    "    #                                      'ctx_value', 'score'])\n",
    "    # recorrer particiones (sub-listas) generadas\n",
    "    for partition in partitions:\n",
    "        # guardar imágenes en una lista\n",
    "        image_bgr_list = open_parallel_bucket(partition, bucket_name)\n",
    "        # se genera DataFrame\n",
    "        prediction_op = prediction(project_id, image_bgr_list.output)\n",
    "        # unión de DataFrames\n",
    "        #df_prediction = pd.concat((df_prediction, prediction_op.output), ignore_index= True)\n",
    "        df_prediction = prediction_op.output\n",
    "        print('se ejecutó partición')\n",
    "    \n",
    "    df_final = table_generation(data_op.output, df_prediction)\n",
    "    save_tables(df_final.output, n_r, project_id, dataset, full_table, random_table, \n",
    "                random_table_backup, prediction_if_exists, random_if_exists, \n",
    "                random_backup_if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f1078-8064-4a84-80d8-cd2199c3c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332983c-3a88-414e-833c-02a9c0881efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "vertex_env",
   "name": "common-cpu.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m99"
  },
  "kernelspec": {
   "display_name": "vertex_env",
   "language": "python",
   "name": "vertex_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

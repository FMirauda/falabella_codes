{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYMyRHDpRVHp",
    "outputId": "6bb75a20-bd90-48f1-9b77-ca410522352f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 266 µs (started: 2022-10-14 15:04:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.97 ms (started: 2022-10-14 15:19:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Query para obtener la data de 3 días antes, considerando que el enlace de la foto, \n",
    "# el nombre del transportista y su rut, no sean nulos\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT DISTINCT transport_ord_id as SOC, i.url as url, shipment.plate_num as plate_num, \n",
    "provider.doc_id as provider_id, \n",
    "provider.doc_verify_digit as provider_verify_digit,\n",
    "provider.name as provider_name, driver.doc_id as driver_id, \n",
    "driver.doc_verify_digit as driver_verify_digit,\n",
    "driver.name as driver_name, driver.last_name as driver_last_name,\n",
    "DATETIME(event_crte_tmst, 'America/Santiago') as event_crte_tmst, dfl_crte_tmst\n",
    "FROM \n",
    "`tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.btd_scha_fal_trmg_api_transport_order_temp`,\n",
    "unnest(image) as i\n",
    " \n",
    "WHERE\n",
    "  i.url is not null\n",
    "  and provider.name is not null\n",
    "  and provider.doc_id is not null\n",
    "  and DATE(event_crte_tmst, 'America/Santiago') = current_date() - 8\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_images = client.query(sql).to_dataframe()\n",
    "\n",
    "print(len(df_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11921\n",
      "time: 14.1 ms (started: 2022-10-14 15:26:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# eliminar filas con urls duplicados\n",
    "df_images = df_images.drop_duplicates(['url'])\n",
    "# resetear indices\n",
    "df_images = df_images.reset_index()\n",
    "# eliminar columna index\n",
    "df_images = df_images.drop(['index'], axis = 1)\n",
    "print(len(df_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas para leer enlaces mas rapido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.72 ms (started: 2022-10-14 15:04:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.52 ms (started: 2022-10-14 12:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "inputs_df = df_images.iloc[0:100].url\n",
    "inputs_list = df_images.iloc[0:10].url.to_list()\n",
    "inputs_array = df_images.iloc[0:10].url.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 208 ms (started: 2022-10-14 15:04:29 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import cv2\n",
    "from tempfile import TemporaryFile\n",
    "import numpy as np\n",
    "\n",
    "# inicialización\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('image_recognition_images')\n",
    "blobs = client.list_blobs(bucket)\n",
    "# generar lista de fotos\n",
    "blob_list = list(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.04 ms (started: 2022-10-14 15:04:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import cv2\n",
    "from tempfile import TemporaryFile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# función para leer imágenes desde el bucket\n",
    "def read_image_bucket(blob_object):\n",
    "    filename = blob_object.name\n",
    "    client = storage.Client()\n",
    "    \n",
    "    bucket = client.get_bucket('image_recognition_images')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    \n",
    "    b = blob.download_as_bytes()\n",
    "    image_cv = np.asarray(bytearray(b), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    area = image.shape[0]*image.shape[1]\n",
    "    url = 'https://prdadessacorptrl.blob.core.windows.net/cl-images/' + filename\n",
    "    #print(image.shape)\n",
    "    #print(area)\n",
    "    \n",
    "    return(image, area, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 ms (started: 2022-10-14 15:04:32 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# funcion para trabajar en paralelo\n",
    "#200.000 imagenes aprox 3 horas\n",
    "# se podria agregar un log cada vez que lee 1000 images y\n",
    "# además ir guardando la lista, de manera de que si se corta\n",
    "# el proceso, se puede retomar desde donde que quedo\n",
    "def open_parallel_bucket(args):\n",
    "    cpus = cpu_count()\n",
    "    results = ThreadPool(cpus - 1).imap_unordered(read_image_bucket, args)\n",
    "    import numpy as np\n",
    "    lista_base = []\n",
    "    for result in results:\n",
    "        lista_base.append(result)\n",
    "    return(lista_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.72 ms (started: 2022-10-14 15:04:34 +00:00)\n"
     ]
    }
   ],
   "source": [
    "len(blob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.58 s (started: 2022-10-14 15:05:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "l_bucket = open_parallel_bucket(blob_list[:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.16 ms (started: 2022-10-14 15:05:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Dataset class para generar los batches\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"Photos prediction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, lista_fotos, normalization = 'mlc', device = 'cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lista_fotos (list): Lista con varias fotos concatenadas y sus areas\n",
    "            normalization (string): Indica si se debe implementar la transformación\n",
    "            de normalización para el clasificador multi-etiqueta ('mlc') o para el \n",
    "            detector de objetos ('ob')\n",
    "            device (string): Indica qué dispositivo se quiere usar: 'cuda' o 'cpu'\n",
    "        \n",
    "        Returns:\n",
    "            pro_img (tensor): Retorna la imagen procesada como tensor lista para la predicción.\n",
    "        \"\"\"\n",
    "        self.lista_fotos = lista_fotos\n",
    "        self.normalization = normalization\n",
    "        self.device = device\n",
    "        # transformación para clasificador\n",
    "        self.mlc_normalization = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                                 ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lista_fotos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        foto = self.lista_fotos[idx][0]\n",
    "        area = self.lista_fotos[idx][1]\n",
    "        url = self.lista_fotos[idx][2]\n",
    "        # aplicar normalizacion\n",
    "        if self.normalization == 'mlc':\n",
    "            pro_img  = self.mlc_normalization(foto)\n",
    "            #print(t_img.shape)\n",
    "            #pro_img = t_img.unsqueeze(dim = 0)\n",
    "            #print(pro_img.shape)\n",
    "        \n",
    "        elif self.normalization == 'od':\n",
    "            t_img_0 = transform.resize(foto, (480, 480))\n",
    "            pro_img = torch.Tensor(t_img_0).permute(2,0,1)\n",
    "\n",
    "        # retornar imagen procesada\n",
    "        return pro_img, area, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.74 ms (started: 2022-10-14 15:05:32 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# se generan batch para clasificador multi-etiqueta y detector de objetos\n",
    "prediction_set_mlc = PredictionDataset(lista_fotos = l_bucket, normalization = 'mlc', device = 'cuda')\n",
    "prediction_set_od = PredictionDataset(lista_fotos = l_bucket, normalization = 'od', device = 'cuda')\n",
    "\n",
    "# para aumentar más el batch_size hay que agregar más memoria\n",
    "predictionLoader_mlc = DataLoader(prediction_set_mlc, batch_size=15, num_workers=2,\n",
    "                         shuffle = False)\n",
    "predictionLoader_od = DataLoader(prediction_set_od, batch_size=15, num_workers=2,\n",
    "                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "# supuesto: tabla con enlaces a cada fotografía\n",
    "\n",
    "def score(url, classificator_1, classificator_2, detector, pesos = {'w_prod': 0.4, 'w_notface': 0.1,\n",
    "                                                 'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2},\n",
    "          thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "                        't_ctx_up': 0.65}, device = 'cuda'):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "      url (str): url donde se encuentra la imagen\n",
    "      classificator_1 (modelo): clasificador multi-etiqueta para detectar la etiqueta del paquete\n",
    "      classificator_2 (modelo): clasificador multi-etiqueta para detectar la cara y el domicilio\n",
    "      detector (modelo): detector de objetos para identificar el paquete y su bbox respectivo\n",
    "      pesos (dict): diccionario con valores para cada peso, es decir,\n",
    "      w_prod (product), w_notface (without face), w_label (product label), \n",
    "      w_num (address number) y w_contx (context)\n",
    "      thresholds = diccionario con valores de umbral para cada criterio, es decir,\n",
    "      t_prod (product), t_face (face detector), t_label (product label) y \n",
    "      t_num (address number)\n",
    "      \n",
    "  Obs: los pesos deben sumar 1 para todos los criterios menos el del numero de domicilio. Este\n",
    "       ultimo corresponde a un beneficio de +0.1 si es que aparece en la fotografía.\n",
    "\n",
    "  Returns:\n",
    "      result_data (dataFrame): cada una de las columnas del dataFrame corresponde a la predicción\n",
    "      de cada criterio sobre cierto umbral, los scores (confianza del modelo) de cada criterio y \n",
    "      la nota de la foto (score).\n",
    "      t_img_0 (tensor): imagen procesada\n",
    "      b (array): bounding boxes de la imagen  \n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.74 ms (started: 2022-10-14 15:05:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def bbox_function(b, area_img):\n",
    "    l =list(range(b.shape[0]))\n",
    "    # recorrer cajas y calcular inter-area para cada combinación\n",
    "    indice = 0\n",
    "    interArea = 0\n",
    "    for i in itertools.combinations(l, r=2):\n",
    "        # obtener coordenadas de la inter-area\n",
    "        x0 = max(b[i[0]][0], b[i[1]][0])\n",
    "        y0 = max(b[i[0]][1], b[i[1]][1])\n",
    "        x1 = min(b[i[0]][2], b[i[1]][2])\n",
    "        y1 = min(b[i[0]][3], b[i[1]][3])\n",
    "\n",
    "        # calcular inter-area\n",
    "        dif_x = x0-x1\n",
    "        dif_y = y0-y1\n",
    "        # se verifica que las esquinas de la interArea esten bien ubicadas \n",
    "        if dif_x < 0 and dif_y < 0:\n",
    "            interArea += dif_x*dif_y\n",
    "            ##interArea += abs(x0-x1)*abs(y0-y1)\n",
    "    \n",
    "    # sumar areas de cada bbox\n",
    "    area_total_bbox = 0\n",
    "    for box in b:\n",
    "        # calcular area de cada bbox\n",
    "        area_bbox = abs(box[0]-box[2])*abs(box[1]-box[3])\n",
    "        # calcular area total de bbox\n",
    "        area_total_bbox += area_bbox\n",
    "\n",
    "    # calcular la union de las areas de cada bbox\n",
    "    union = area_total_bbox - interArea\n",
    "\n",
    "    # calcular contexto\n",
    "    if torch.is_tensor(union):\n",
    "        contexto = union.item()/area_img\n",
    "    \n",
    "    else:\n",
    "        contexto = union/area_img\n",
    "    \n",
    "    return contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.29 s (started: 2022-10-14 15:09:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2}\n",
    "\n",
    "thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "              't_ctx_up': 0.65}\n",
    "\n",
    "device = 'cuda'\n",
    "# evaluar con gpu o cpu \n",
    "device = torch.device(device)\n",
    "classificator_1 = ml_model_1 \n",
    "classificator_2 = ml_model_2\n",
    "detector = od_model\n",
    "\n",
    "classificator_1.to(device)\n",
    "classificator_1.eval()  \n",
    "classificator_2.to(device)\n",
    "classificator_2.eval()           \n",
    "detector.to(device)\n",
    "detector.eval()\n",
    "df_base = pd.DataFrame(columns = ['url','paquete', 's_paquete', 'etiqueta_producto',\n",
    "                                      's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\n",
    "                                      'numero_domicilio', 's_numero_domicilio', 'contexto',\n",
    "                                      'ctx_value', 'score'])\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(zip(predictionLoader_mlc, predictionLoader_od)):\n",
    "        # se obtiene la imagen de cada dataLoader\n",
    "        image_mlc = image[0][0]\n",
    "        image_od = image[1][0]\n",
    "        \n",
    "        # obtener areas y urls de las imagenes\n",
    "        area_img = image[0][1]\n",
    "        #img_url = np.asarray(image[0][2]).reshape(-1,1)\n",
    "        img_url = list(image[0][2])\n",
    "\n",
    "        # CLASIFICACION MULTI-ETIQUETA\n",
    "        #---------------------------clasificador 1--------------------------------\n",
    "        label_base = classificator_1(image_mlc.to(device))\n",
    "        #---------------------------clasificador 2--------------------------------\n",
    "        label_etiqueta = classificator_2(image_mlc.to(device))\n",
    "        # obtener score\n",
    "        score_etiqueta = torch.sigmoid(label_etiqueta['label']).squeeze()\n",
    "        score = torch.sigmoid(label_base['label']).squeeze()\n",
    "        ##print('score:',score.shape)\n",
    "        # obtener predicción etiqueta\n",
    "        s_etiqueta = score_etiqueta[:,0]\n",
    "        etiqueta = torch.where(s_etiqueta >= thresholds['t_label'], 1, 0)\n",
    "        # obtener predicción domicilio\n",
    "        s_domicilio = score[:,1]\n",
    "        domicilio = torch.where(s_domicilio >= thresholds['t_num'], 1, 0)\n",
    "        # obtener predicción cara\n",
    "        s_cara = score[:,2]\n",
    "        s_no_cara = 1-s_cara\n",
    "        no_cara = torch.where(s_no_cara >= thresholds['t_face'], 1, 0)\n",
    "        \n",
    "        etiqueta = etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "        s_etiqueta = s_etiqueta.reshape(-1,1).to('cpu').numpy()\n",
    "        domicilio = domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "        s_domicilio = s_domicilio.reshape(-1,1).to('cpu').numpy()\n",
    "        no_cara = no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "        s_no_cara = s_no_cara.reshape(-1,1).to('cpu').numpy()\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # DETECTOR DE OBJETOS\n",
    "        od_prediction = od_model(image_od.to(device))\n",
    "        ##print(od_prediction)\n",
    "        dataFrame = pd.DataFrame(od_prediction).to_dict(orient=\"list\")\n",
    "        #print(dataFrame['boxes'])\n",
    "        #print(torch.Tensor(dataFrame['boxes']))\n",
    "        list_of_lists = list(map(lambda x: x.tolist(), dataFrame['boxes']))\n",
    "        array_of_arrays1= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['boxes'])))\n",
    "        array_of_arrays2= np.array(list(map(lambda x: x.to('cpu').numpy(), dataFrame['scores'])))\n",
    "        #bla = list(map(lambda x: torch.where(x >= 0.5, 1, 0), dataFrame['scores']))\n",
    "        bla2 = np.array(list(map(lambda x: np.where(x.to('cpu').numpy()>=thresholds['t_prod'],1,\n",
    "                                                    0).reshape(-1,1),\n",
    "                                 dataFrame['scores'])))\n",
    "        #print(array_of_arrays1)\n",
    "        #print(bla2)\n",
    "        producto = array_of_arrays1*bla2\n",
    "        #print(producto)\n",
    "        producto_wz = np.array(list(map(lambda x: x[~np.all(x == 0, axis=1)], producto)))\n",
    "        contexto = np.array(list(map(lambda x,y: bbox_function(x,y), producto_wz, area_img)))\n",
    "        contexto = contexto.reshape(-1,1)\n",
    "        ctx_value = np.where(((contexto >= thresholds['t_ctx_down']) & (contexto <= thresholds['t_ctx_up'])),\n",
    "                             1, 0) \n",
    "        #print(ctx_value)\n",
    "        #print(array_of_arrays*array_of_arrays)\n",
    "        #print(array_of_arrays2)\n",
    "        #print(np.where(array_of_arrays2.all() < 0.5, 0, array_of_arrays2))\n",
    "        #print(list(map(lambda x: max(x), dataFrame['boxes'])))\n",
    "        s_paquete = torch.tensor(list(map(lambda x: max(x, default=0), dataFrame['scores'])))\n",
    "        s_paquete = s_paquete.reshape(-1,1)\n",
    "        #printtorch.tensor(dataFrame['scores'])\n",
    "        #print(scores.reshape(-1,1).to('cpu').numpy())\n",
    "        paquete = torch.where(s_paquete >= thresholds['t_prod'], 1, 0)\n",
    "        #############listos con score!!#################3\n",
    "        # que pasa si no se puede leer el url?\n",
    "        # que pasa si la imagen no tiene bbox? -> funciona todo bien\n",
    "        # falta enlace: corecto o incorrecto\n",
    "        \n",
    "        result = np.concatenate([s_paquete, paquete, etiqueta, s_etiqueta, no_cara, s_no_cara,\n",
    "                                domicilio, s_domicilio, contexto, ctx_value], axis = 1)\n",
    "        \n",
    "        ##print(result)\n",
    "        df = pd.DataFrame(result, columns=['paquete','s_paquete','etiqueta_producto', \\\n",
    "                                                       's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\\\n",
    "                                                       'numero_domicilio', 's_numero_domicilio', 'contexto',\\\n",
    "                                                       'ctx_value'])\n",
    "        \n",
    "        pesos = {'w_prod': 0.4, 'w_notface': 0.1,'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2}\n",
    "        df['score'] = df.paquete*pesos['w_prod'] + df.etiqueta_producto*pesos['w_label'] + \\\n",
    "        df.sin_rostro*pesos['w_notface'] + df.numero_domicilio*pesos['w_num'] + df.ctx_value*pesos['w_contx']\n",
    "        #FALTA GUARDAR EL ENLACE Y NOTIFICAR CUANDO UN ENLACE ESTA ERRONEO\n",
    "        #df['url'] = img_url\n",
    "        df.insert(loc=0, column='url', value=img_url)\n",
    "        # concatenar a df anterior\n",
    "        df_base = pd.concat((df_base,df), ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>paquete</th>\n",
       "      <th>s_paquete</th>\n",
       "      <th>etiqueta_producto</th>\n",
       "      <th>s_etiqueta_producto</th>\n",
       "      <th>sin_rostro</th>\n",
       "      <th>s_sin_rostro</th>\n",
       "      <th>numero_domicilio</th>\n",
       "      <th>s_numero_domicilio</th>\n",
       "      <th>contexto</th>\n",
       "      <th>ctx_value</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.963417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127961</td>\n",
       "      <td>4.978457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990142</td>\n",
       "      <td>5.323792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>3.571784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.978356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>2.793778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.996716</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>1.315118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999389</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.289817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.990149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041577</td>\n",
       "      <td>2.138992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.998940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>2.291763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.983730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056980</td>\n",
       "      <td>0.726401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.393492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016738</td>\n",
       "      <td>0.645822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020730</td>\n",
       "      <td>3.532650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.214798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>2.835137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>2.420716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>1.564659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>4.140698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023492</td>\n",
       "      <td>2.817755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025386</td>\n",
       "      <td>0.722593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.963004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>2.551775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024940</td>\n",
       "      <td>3.206243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.976794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499690</td>\n",
       "      <td>1.872654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.992826</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>6.504009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.997834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028850</td>\n",
       "      <td>0.358599</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.994710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032327</td>\n",
       "      <td>5.370724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714907</td>\n",
       "      <td>0.254555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.983841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111714</td>\n",
       "      <td>1.628018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.999138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953157</td>\n",
       "      <td>1.751860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.997339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.984726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328409</td>\n",
       "      <td>2.284904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.995319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090887</td>\n",
       "      <td>1.567321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>2.539405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.921893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>3.221899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.880033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037376</td>\n",
       "      <td>2.073478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.923158</td>\n",
       "      <td>0.644985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.099323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://prdadessacorptrl.blob.core.windows.net...</td>\n",
       "      <td>0.996687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.932787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>1.247824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url   paquete  s_paquete  \\\n",
       "0   https://prdadessacorptrl.blob.core.windows.net...  0.963417        1.0   \n",
       "1   https://prdadessacorptrl.blob.core.windows.net...  0.999699        1.0   \n",
       "2   https://prdadessacorptrl.blob.core.windows.net...  0.999606        1.0   \n",
       "3   https://prdadessacorptrl.blob.core.windows.net...  0.999753        1.0   \n",
       "4   https://prdadessacorptrl.blob.core.windows.net...  0.996716        1.0   \n",
       "5   https://prdadessacorptrl.blob.core.windows.net...  0.999389        1.0   \n",
       "6   https://prdadessacorptrl.blob.core.windows.net...  0.990149        1.0   \n",
       "7   https://prdadessacorptrl.blob.core.windows.net...  0.998940        1.0   \n",
       "8   https://prdadessacorptrl.blob.core.windows.net...  0.983730        1.0   \n",
       "9   https://prdadessacorptrl.blob.core.windows.net...  0.998970        1.0   \n",
       "10  https://prdadessacorptrl.blob.core.windows.net...  0.999577        1.0   \n",
       "11  https://prdadessacorptrl.blob.core.windows.net...  0.892987        1.0   \n",
       "12  https://prdadessacorptrl.blob.core.windows.net...  0.999691        1.0   \n",
       "13  https://prdadessacorptrl.blob.core.windows.net...  0.998644        1.0   \n",
       "14  https://prdadessacorptrl.blob.core.windows.net...  0.999219        1.0   \n",
       "15  https://prdadessacorptrl.blob.core.windows.net...  0.999043        1.0   \n",
       "16  https://prdadessacorptrl.blob.core.windows.net...  0.999675        1.0   \n",
       "17  https://prdadessacorptrl.blob.core.windows.net...  0.999212        1.0   \n",
       "18  https://prdadessacorptrl.blob.core.windows.net...  0.963004        1.0   \n",
       "19  https://prdadessacorptrl.blob.core.windows.net...  0.999034        1.0   \n",
       "20  https://prdadessacorptrl.blob.core.windows.net...  0.976794        1.0   \n",
       "21  https://prdadessacorptrl.blob.core.windows.net...  0.992826        1.0   \n",
       "22  https://prdadessacorptrl.blob.core.windows.net...  0.997834        1.0   \n",
       "23  https://prdadessacorptrl.blob.core.windows.net...  0.994710        1.0   \n",
       "24  https://prdadessacorptrl.blob.core.windows.net...  0.979354        1.0   \n",
       "25  https://prdadessacorptrl.blob.core.windows.net...  0.983841        1.0   \n",
       "26  https://prdadessacorptrl.blob.core.windows.net...  0.999138        1.0   \n",
       "27  https://prdadessacorptrl.blob.core.windows.net...  0.000000        0.0   \n",
       "28  https://prdadessacorptrl.blob.core.windows.net...  0.997339        1.0   \n",
       "29  https://prdadessacorptrl.blob.core.windows.net...  0.995319        1.0   \n",
       "30  https://prdadessacorptrl.blob.core.windows.net...  0.998871        1.0   \n",
       "31  https://prdadessacorptrl.blob.core.windows.net...  0.921893        1.0   \n",
       "32  https://prdadessacorptrl.blob.core.windows.net...  0.880033        1.0   \n",
       "33  https://prdadessacorptrl.blob.core.windows.net...  0.998307        1.0   \n",
       "34  https://prdadessacorptrl.blob.core.windows.net...  0.996687        1.0   \n",
       "\n",
       "    etiqueta_producto  s_etiqueta_producto  sin_rostro  s_sin_rostro  \\\n",
       "0                 0.0             0.000099         1.0      0.993524   \n",
       "1                 1.0             1.000000         1.0      0.979436   \n",
       "2                 1.0             1.000000         1.0      0.993501   \n",
       "3                 1.0             0.999999         1.0      0.978356   \n",
       "4                 1.0             0.990608         1.0      0.995704   \n",
       "5                 1.0             1.000000         1.0      0.986731   \n",
       "6                 1.0             0.955069         1.0      0.982836   \n",
       "7                 1.0             1.000000         1.0      0.978994   \n",
       "8                 0.0             0.002999         0.0      0.001072   \n",
       "9                 1.0             0.999981         1.0      0.992933   \n",
       "10                1.0             0.999998         1.0      0.996734   \n",
       "11                0.0             0.041375         1.0      0.973333   \n",
       "12                1.0             1.000000         0.0      0.107794   \n",
       "13                1.0             0.999999         0.0      0.423579   \n",
       "14                1.0             1.000000         1.0      0.993086   \n",
       "15                1.0             1.000000         1.0      0.967516   \n",
       "16                1.0             1.000000         1.0      0.996079   \n",
       "17                1.0             0.999555         1.0      0.989971   \n",
       "18                0.0             0.002214         1.0      0.899131   \n",
       "19                1.0             1.000000         1.0      0.990646   \n",
       "20                1.0             1.000000         1.0      0.951649   \n",
       "21                1.0             1.000000         1.0      0.994000   \n",
       "22                1.0             1.000000         0.0      0.250218   \n",
       "23                1.0             1.000000         1.0      0.991905   \n",
       "24                0.0             0.122541         1.0      0.663819   \n",
       "25                0.0             0.000090         1.0      0.960205   \n",
       "26                1.0             1.000000         1.0      0.985803   \n",
       "27                0.0             0.028860         1.0      0.982300   \n",
       "28                0.0             0.033908         1.0      0.984726   \n",
       "29                0.0             0.008112         1.0      0.880340   \n",
       "30                1.0             1.000000         1.0      0.988304   \n",
       "31                0.0             0.001168         0.0      0.004663   \n",
       "32                0.0             0.005250         0.0      0.349358   \n",
       "33                1.0             1.000000         1.0      0.898351   \n",
       "34                1.0             0.932787         1.0      0.995951   \n",
       "\n",
       "    numero_domicilio  s_numero_domicilio  contexto  ctx_value     score  \n",
       "0                0.0            0.127961  4.978457        0.0  0.485367  \n",
       "1                1.0            0.990142  5.323792        0.0  0.899880  \n",
       "2                0.0            0.020523  3.571784        0.0  0.799842  \n",
       "3                0.0            0.006682  2.793778        0.0  0.799901  \n",
       "4                0.0            0.000852  1.315118        0.0  0.798687  \n",
       "5                0.0            0.002695  0.289817        1.0  0.999756  \n",
       "6                0.0            0.041577  2.138992        0.0  0.796060  \n",
       "7                0.0            0.005753  2.291763        0.0  0.799576  \n",
       "8                0.0            0.056980  0.726401        0.0  0.393492  \n",
       "9                0.0            0.016738  0.645822        1.0  0.999588  \n",
       "10               0.0            0.020730  3.532650        0.0  0.799831  \n",
       "11               1.0            0.999446  0.214798        1.0  0.757195  \n",
       "12               0.0            0.005744  2.835137        0.0  0.699876  \n",
       "13               0.0            0.002147  2.420716        0.0  0.699458  \n",
       "14               0.0            0.023015  1.564659        0.0  0.799688  \n",
       "15               0.0            0.009089  4.140698        0.0  0.799617  \n",
       "16               0.0            0.023492  2.817755        0.0  0.799870  \n",
       "17               0.0            0.025386  0.722593        0.0  0.799685  \n",
       "18               0.0            0.001198  2.551775        0.0  0.485202  \n",
       "19               0.0            0.024940  3.206243        0.0  0.799614  \n",
       "20               0.0            0.499690  1.872654        0.0  0.790718  \n",
       "21               0.0            0.005256  6.504009        0.0  0.797130  \n",
       "22               0.0            0.028850  0.358599        1.0  0.899134  \n",
       "23               0.0            0.032327  5.370724        0.0  0.797884  \n",
       "24               1.0            0.714907  0.254555        1.0  0.791741  \n",
       "25               0.0            0.111714  1.628018        0.0  0.493536  \n",
       "26               1.0            0.953157  1.751860        0.0  0.899655  \n",
       "27               0.0            0.096425  0.000000        0.0  0.100000  \n",
       "28               0.0            0.328409  2.284904        0.0  0.498936  \n",
       "29               0.0            0.090887  1.567321        0.0  0.498128  \n",
       "30               0.0            0.041420  2.539405        0.0  0.799548  \n",
       "31               0.0            0.010200  3.221899        0.0  0.368757  \n",
       "32               0.0            0.037376  2.073478        0.0  0.352013  \n",
       "33               1.0            0.923158  0.644985        1.0  1.099323  \n",
       "34               0.0            0.000350  1.247824        0.0  0.798675  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.5 ms (started: 2022-10-14 15:09:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.83 ms (started: 2022-10-14 15:26:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "inner_merged = pd.merge(df_images, df_base, on=[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOC</th>\n",
       "      <th>url</th>\n",
       "      <th>plate_num</th>\n",
       "      <th>provider_id</th>\n",
       "      <th>provider_verify_digit</th>\n",
       "      <th>provider_name</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>driver_verify_digit</th>\n",
       "      <th>driver_name</th>\n",
       "      <th>driver_last_name</th>\n",
       "      <th>...</th>\n",
       "      <th>s_paquete</th>\n",
       "      <th>etiqueta_producto</th>\n",
       "      <th>s_etiqueta_producto</th>\n",
       "      <th>sin_rostro</th>\n",
       "      <th>s_sin_rostro</th>\n",
       "      <th>numero_domicilio</th>\n",
       "      <th>s_numero_domicilio</th>\n",
       "      <th>contexto</th>\n",
       "      <th>ctx_value</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SOC, url, plate_num, provider_id, provider_verify_digit, provider_name, driver_id, driver_verify_digit, driver_name, driver_last_name, event_crte_tmst, dfl_crte_tmst, paquete, s_paquete, etiqueta_producto, s_etiqueta_producto, sin_rostro, s_sin_rostro, numero_domicilio, s_numero_domicilio, contexto, ctx_value, score]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.9 ms (started: 2022-10-14 15:26:19 +00:00)\n"
     ]
    }
   ],
   "source": [
    "inner_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "# supuesto: tabla con enlaces a cada fotografía\n",
    "\n",
    "def score(url, classificator_1, classificator_2, detector, pesos = {'w_prod': 0.4, 'w_notface': 0.1,\n",
    "                                                 'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2},\n",
    "          thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "                        't_ctx_up': 0.65}, device = 'cuda'):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "      url (str): url donde se encuentra la imagen\n",
    "      classificator_1 (modelo): clasificador multi-etiqueta para detectar la etiqueta del paquete\n",
    "      classificator_2 (modelo): clasificador multi-etiqueta para detectar la cara y el domicilio\n",
    "      detector (modelo): detector de objetos para identificar el paquete y su bbox respectivo\n",
    "      pesos (dict): diccionario con valores para cada peso, es decir,\n",
    "      w_prod (product), w_notface (without face), w_label (product label), \n",
    "      w_num (address number) y w_contx (context)\n",
    "      thresholds = diccionario con valores de umbral para cada criterio, es decir,\n",
    "      t_prod (product), t_face (face detector), t_label (product label) y \n",
    "      t_num (address number)\n",
    "      \n",
    "  Obs: los pesos deben sumar 1 para todos los criterios menos el del numero de domicilio. Este\n",
    "       ultimo corresponde a un beneficio de +0.1 si es que aparece en la fotografía.\n",
    "\n",
    "  Returns:\n",
    "      result_data (dataFrame): cada una de las columnas del dataFrame corresponde a la predicción\n",
    "      de cada criterio sobre cierto umbral, los scores (confianza del modelo) de cada criterio y \n",
    "      la nota de la foto (score).\n",
    "      t_img_0 (tensor): imagen procesada\n",
    "      b (array): bounding boxes de la imagen  \n",
    "  \"\"\"\n",
    "  # evaluar con gpu o cpu \n",
    "  device = torch.device(device)\n",
    "\n",
    "  classificator_1.to(device)\n",
    "  classificator_1.eval()  \n",
    "  classificator_2.to(device)\n",
    "  classificator_2.eval()           \n",
    "  detector.to(device)\n",
    "  detector.eval()\n",
    "  # transformación para clasificador\n",
    "  transformations = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                                 ])\n",
    "  with torch.no_grad():\n",
    "    # Obtener imagen\n",
    "    try:\n",
    "        url_open = urllib.request.urlopen(url)\n",
    "        image_cv = np.asarray(bytearray(url_open.read()), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # CLASIFICADOR MULTI-ETIQUETA\n",
    "        # transformación para aplicar clasificador\n",
    "        t_img = transformations(image)\n",
    "        img = t_img.unsqueeze(dim = 0)\n",
    "        # obtener clasificación\n",
    "        #---------------------------clasificador 1--------------------------------\n",
    "        label_base = classificator_1(img.to(device))\n",
    "        #---------------------------clasificador 2--------------------------------\n",
    "        label_etiqueta = classificator_2(img.to(device))\n",
    "        # obtener score\n",
    "        score = torch.sigmoid(label_base['label']).squeeze()\n",
    "        score_etiqueta = torch.sigmoid(label_etiqueta['label']).squeeze()\n",
    "        # analizar score para 'etiqueta'\n",
    "        s_etiqueta = score_etiqueta[0].item()\n",
    "        if s_etiqueta >= thresholds['t_label']:\n",
    "          etiqueta = 1\n",
    "        else:\n",
    "          etiqueta = 0\n",
    "        # analizar score para 'n° domicilio'\n",
    "        s_domicilio = score[1].item()\n",
    "        if s_domicilio >= thresholds['t_num']:\n",
    "          domicilio = 1\n",
    "        else:\n",
    "          domicilio = 0\n",
    "        # analizar score para 'cara'\n",
    "        s_cara = score[2].item()\n",
    "        s_no_cara = 1-s_cara\n",
    "        if s_cara >= thresholds['t_face']:\n",
    "          no_cara = 0\n",
    "        else:\n",
    "          no_cara = 1\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # DETECTOR DE OBJETOS\n",
    "        # transformación para aplicar detector\n",
    "        t_img_0 = transform.resize(image, (480, 480))\n",
    "        # obtener area de la imagen\n",
    "        area_img = t_img_0.shape[0]*t_img_0.shape[1]\n",
    "        t_img = torch.Tensor(t_img_0).permute(2,0,1).to(device)\n",
    "        od_prediction = od_model([t_img])\n",
    "        \n",
    "        # analizar score para 'paquete'\n",
    "        s_paquete = max(od_prediction[0]['scores']).item()\n",
    "        if od_prediction[0]['labels'].shape[0] > 0 and \\\n",
    "        s_paquete >= thresholds['t_prod']:\n",
    "          paquete = 1\n",
    "        else:\n",
    "          paquete = 0 \n",
    "        # determinar unión de todos los bbox que encierren a un paquete (score >= t_prod) \n",
    "        boxes = od_prediction[0]['boxes'].to(device)\n",
    "        scores = od_prediction[0]['scores'].to(device)\n",
    "        scores = torch.where(scores >= thresholds['t_prod'], 1, 0)\n",
    "        scores = torch.unsqueeze(scores, dim=0)\n",
    "        scores_t = torch.transpose(scores, 0, 1)\n",
    "        mult = torch.mul(boxes, scores_t)\n",
    "        b = mult[mult.sum(dim=1) != 0] # eliminar filas con puros ceros\n",
    "        l =list(range(b.size()[0]))\n",
    "        # recorrer cajas y calcular inter-area para cada combinación\n",
    "        indice = 0\n",
    "        interArea = 0\n",
    "        for i in itertools.combinations(l, r=2):\n",
    "          # obtener coordenadas de la inter-area\n",
    "          x0 = max(b[i[0]][0], b[i[1]][0])\n",
    "          y0 = max(b[i[0]][1], b[i[1]][1])\n",
    "          x1 = min(b[i[0]][2], b[i[1]][2])\n",
    "          y1 = min(b[i[0]][3], b[i[1]][3])\n",
    "    \n",
    "          # calcular inter-area\n",
    "          dif_x = x0-x1\n",
    "          dif_y = y0-y1\n",
    "          # se verifica que las esquinas de la interArea esten bien ubicadas \n",
    "          if dif_x < 0 and dif_y < 0:\n",
    "            interArea += dif_x*dif_y\n",
    "          ##interArea += abs(x0-x1)*abs(y0-y1)\n",
    "      \n",
    "        # sumar areas de cada bbox\n",
    "        area_total_bbox = 0\n",
    "        for box in b:\n",
    "          # calcular area de cada bbox\n",
    "          area_bbox = abs(box[0]-box[2])*abs(box[1]-box[3])\n",
    "          # calcular area total de bbox\n",
    "          area_total_bbox += area_bbox\n",
    "    \n",
    "        # calcular la union de las areas de cada bbox\n",
    "        union = area_total_bbox - interArea\n",
    "    \n",
    "        # calcular contexto\n",
    "        if torch.is_tensor(union):\n",
    "          contexto = union.item()/area_img\n",
    "      \n",
    "        else:\n",
    "          contexto = union/area_img\n",
    "        \n",
    "        # analizar contexto \n",
    "        if (contexto >= thresholds['t_ctx_down']) and (contexto <= thresholds['t_ctx_up']):\n",
    "          ctx_value = 1\n",
    "    \n",
    "        else:\n",
    "          ctx_value = 0\n",
    "      \n",
    "        # calcular score\n",
    "        score = pesos['w_prod']*paquete + pesos['w_label']*etiqueta + \\\n",
    "        pesos['w_notface']*no_cara + pesos['w_num']*domicilio + pesos['w_contx']*ctx_value\n",
    "        \n",
    "        enlace = 'correcto'\n",
    "    \n",
    "    except:\n",
    "        paquete = None\n",
    "        s_paquete = None\n",
    "        etiqueta = None\n",
    "        s_etiqueta = None\n",
    "        no_cara = None\n",
    "        s_no_cara = None\n",
    "        domicilio = None\n",
    "        s_domicilio = None\n",
    "        contexto = None\n",
    "        ctx_value = None\n",
    "        score = None\n",
    "        t_img_0 = None\n",
    "        b = None\n",
    "        enlace = 'incorrecto'\n",
    "        \n",
    "    # Vector de salida (fila) Prob. Etiqueta producto, Etiqueta producto, Prob. Numero domicilio, Numero domicilio\n",
    "    ##result = np.array([paquete, etiqueta, no_cara, domicilio, contexto, ctx_value, score])\n",
    "    result = np.array([paquete, s_paquete, etiqueta, s_etiqueta, no_cara, s_no_cara, domicilio,\\\n",
    "                       s_domicilio, contexto, ctx_value, score, enlace])\n",
    "    # Se hace un dataframe\n",
    "    ##result_data = pd.DataFrame([result], [0], columns=['paquete', 'etiqueta_producto', 'sin_rostro',\\\n",
    "    ##                                              'numero_domicilio', 'contexto', 'ctx_value', 'score'])\n",
    "    result_data = pd.DataFrame([result], [0], columns=['paquete', 's_paquete', 'etiqueta_producto', \\\n",
    "                                                       's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\\\n",
    "                                                       'numero_domicilio', 's_numero_domicilio', 'contexto',\\\n",
    "                                                       'ctx_value', 'score', 'enlace'])\n",
    "    \n",
    "    return result_data, t_img_0, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.84 ms (started: 2022-10-05 18:18:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# numero de kernels\n",
    "nk = 2\n",
    "# escoger valores unicos de SOC\n",
    "soc = df_images.SOC.unique()\n",
    "# dividirlos en nk partes (para los nk kernels)\n",
    "import numpy as np\n",
    "partitions = np.array_split(soc, nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 955 µs (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# seleccionar de manera random 2000 soc de la segunda partición\n",
    "# de modo que al final se obtengan alrededor de 5000 filas\n",
    "# por kernel\n",
    "from numpy import random\n",
    "partitions2 = random.choice(partitions[1], size=2000, replace=False, p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.75 ms (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# se obtienen las filas que estan en la partición \n",
    "df_images2 = df_images.loc[df_images.SOC.isin(partitions2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.61 ms (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# se resetea el indice y se elimina la columna 'index'\n",
    "df_images2 = df_images2.reset_index().drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.16 ms (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# imprimir largo dataset\n",
    "len(df_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 458 µs (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# prueba\n",
    "#sql = \"\"\"\n",
    "#SELECT transport_ord_id as SOC, i.url as url, shipment.plate_num as plate_num, \n",
    "#provider.doc_id as provider_id, \n",
    "#provider.doc_verify_digit as provider_verify_digit,\n",
    "#provider.name as provider_name, driver.doc_id as driver_id, \n",
    "#driver.doc_verify_digit as driver_verify_digit,\n",
    "#driver.name as driver_name, driver.last_name as driver_last_name,\n",
    "#event_crte_tmst, dfl_crte_tmst\n",
    "#FROM \n",
    "#`tc-sc-bi-bigdata-corp-tsod-dev.image_recognition.btd_scha_fal_trmg_api_transport_order`,\n",
    "#unnest(image) as i\n",
    "#\n",
    "#LIMIT 500\n",
    "#\"\"\"\n",
    "#\n",
    "#df_images = client.query(sql).to_dataframe()\n",
    "#\n",
    "#df_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.91 ms (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df = df_images2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOC                      False\n",
       "url                      False\n",
       "plate_num                False\n",
       "provider_id              False\n",
       "provider_verify_digit     True\n",
       "provider_name            False\n",
       "driver_id                 True\n",
       "driver_verify_digit       True\n",
       "driver_name               True\n",
       "driver_last_name          True\n",
       "event_crte_tmst          False\n",
       "dfl_crte_tmst            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.02 ms (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKOThTSiRQbs",
    "outputId": "170aad89-9ea4-41b3-a710-ba6e506d30ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 197 µs (started: 2022-10-05 12:53:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#!pip install ipython-autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUSQ0_cQZ_EG"
   },
   "source": [
    "# Cargar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGcgcEAQgM91",
    "outputId": "463e3ad7-5913-402e-a97e-a4a412f4a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 286 µs (started: 2022-10-07 11:55:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_vxYx-kqWM9",
    "outputId": "fd719451-d280-48be-c9e8-0dbe14216e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 560 µs (started: 2022-10-13 19:44:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "\n",
    "#!pip install albumentations\n",
    "#!pip install pycocotools --quiet\n",
    "\n",
    "# Clone TorchVision repo and copy helper files\n",
    "#!git clone https://github.com/pytorch/vision.git\n",
    "#%cd vision\n",
    "#!git checkout v0.3.0\n",
    "#%cd ..\n",
    "#!cp vision/references/detection/utils.py ./\n",
    "#!cp vision/references/detection/transforms.py ./\n",
    "#!cp vision/references/detection/coco_eval.py ./\n",
    "#!cp vision/references/detection/engine.py ./\n",
    "#!cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JP5zwN_ef-pS",
    "outputId": "a724ab1d-31ea-4a13-c90b-5dc4b797a470"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7ff7e33ccf10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 s (started: 2022-10-14 15:04:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# imporar librerias\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms \n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet101\n",
    "import urllib.request\n",
    "import cv2\n",
    "\n",
    "from ignite.metrics import ClassificationReport\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "\n",
    "from genericpath import exists\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wZHSTyIqRrv",
    "outputId": "62f2acc8-5c73-417e-8e57-497223d12ec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 113 ms (started: 2022-10-14 15:04:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "a4d602abedbf472ba8443b587b7172a5",
      "e369973d094144a583efe831a749932e",
      "f185edec20214d4f9174944db636ce7b",
      "05887b1a958f4535938c3a9208b4ebaf",
      "a23ac5b3ea2d4bd9950f901288642e51",
      "d482f72576e946f59c20fea52f2973c3",
      "a0c5367310094e45baa5e4f21daa819a",
      "f3c586b991004e839c47ee46163195e8",
      "78b8a851199c421694811a3d2b09d77a",
      "9597f48902e54c7a9989484bcf6a239d",
      "4a4aad02b8d24bffa799e6a5fdd4e006"
     ]
    },
    "id": "HJ34_CvLZ1Ev",
    "outputId": "5405e17e-d28e-4ce8-db14-fa922c63a3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.97 s (started: 2022-10-14 15:04:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnext101_32x8d\n",
    "resnext1 = resnext101_32x8d(pretrained=True)\n",
    "resnext2 = resnext101_32x8d(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R92bFuyRgJyX",
    "outputId": "450b4d59-64ec-47cc-ceb8-aa6f601d110e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 932 µs (started: 2022-10-14 15:04:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Cargar backbone resnet y resnext\n",
    "class MultilabelClassifier1(nn.Module):\n",
    "    def __init__(self, n_classes, pretrain_model):\n",
    "        super().__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.pretrain_model.children())[:-1]))\n",
    "\n",
    "        self.classes = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=2048, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'label': self.classes(x)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS4Uf4A_gVOc",
    "outputId": "8845efa9-ce21-4a88-e9d8-6c92c50464df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.17 s (started: 2022-10-14 15:04:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo para cara y n° domicilio\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "PATH_1 =  '/home/jupyter/Score/Modelos/mlc_model_4.pth'\n",
    "ml_model_1 = MultilabelClassifier1(3, resnext1).to(device)\n",
    "ml_model_dict_1 = torch.load(PATH_1, map_location=torch.device(device))\n",
    "ml_model_1.load_state_dict(ml_model_dict_1['model_state_dict'])\n",
    "#ml_model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mu9LDubjFYMP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.648120448645789"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.65 ms (started: 2022-10-14 15:04:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ml_model_dict_1['validation loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkSUyeTFFa-a",
    "outputId": "5c9f2b62-3e24-4cda-dbca-2fd21f2da7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "time: 441 µs (started: 2022-10-14 15:04:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# obtener epoca óptima\n",
    "optimal_epoch = ml_model_dict_1['epoch']\n",
    "print(optimal_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tm-CvmiBXT3w",
    "outputId": "28e32eb8-7a55-4b9c-d1cd-d929864babc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 753 µs (started: 2022-10-14 15:04:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Cargar backbone -> para modelo 'mlc_model_baseline_data_revisada_1_2.pth'\n",
    "class MultilabelClassifier2(nn.Module):\n",
    "    def __init__(self, n_classes, pretrain_model):\n",
    "        super().__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.pretrain_model.children())[:-1]))\n",
    "\n",
    "        self.classes = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=2048, out_features=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'label': self.classes(x)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fm-CYGI1XCXP",
    "outputId": "162dbb7e-6eaa-4013-ea2b-70341e55a973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.84 s (started: 2022-10-14 15:04:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo para etiqueta del producto\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "PATH_2 = '/home/jupyter/Score/Modelos/mlc_model_baseline_data_revisada_1_2.pth'\n",
    "ml_model_2 = MultilabelClassifier2(3, resnext2).to(device)\n",
    "ml_model_dict_2 = torch.load(PATH_2, map_location=torch.device(device))\n",
    "ml_model_2.load_state_dict(ml_model_dict_2['model_state_dict'])\n",
    "#ml_model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh0l415kFkk6",
    "outputId": "8b0071e5-bb2e-4505-cdb6-9e9ee43be2d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0754197467943256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.6 ms (started: 2022-10-14 15:04:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ml_model_dict_2['validation loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMKYd-uWFm9-",
    "outputId": "d8ce9498-5983-4cf8-ad6c-04727ed08573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "time: 606 µs (started: 2022-10-14 15:04:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# obtener epoca óptima\n",
    "optimal_epoch = ml_model_dict_2['epoch']\n",
    "print(optimal_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt2NjM5_p-Ag",
    "outputId": "a776eb5c-2e02-4842-b2bd-da49c11d646c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.22 ms (started: 2022-10-14 15:04:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# REFERENCIAR!\n",
    "# función de ayuda para cargar modelo\n",
    "def get_object_detection_model(num_classes):\n",
    "  # load a model pre-trained on COCO\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  # get number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "32a0c7587cf74eeb98287fdf4767f938",
      "e932cf1b4e484520a4dc4dd6672a7f17",
      "e6dd759dedd84fe3afde7fafadbdc5e6",
      "074323727f34416c895035b6d28e3738",
      "43204103508c4482921d3f6bd45fe366",
      "4271f826e55c4fee959a8097d9695e36",
      "97e08226970849feaea2b8c4a2c59162",
      "23c9ac6aff0448a2a69bf53e98c1b64a",
      "1d7e20161e9248e6a41c541517122a17",
      "a272d658182140b29fe755334af326c0",
      "758e0c34b07e4c168617e8857e09b8fa"
     ]
    },
    "id": "tvizBRhSpzyU",
    "outputId": "0246a34e-d5ce-4134-abf5-74e7a890bbf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 940 ms (started: 2022-10-14 15:04:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo detector de objetos\n",
    "device = 'cuda'\n",
    "#device = 'cpu'\n",
    "model_path = '/home/jupyter/Score/Modelos/model_cf_12y3_1.pth'\n",
    "num_classes = 2\n",
    "od_model = get_object_detection_model(num_classes)\n",
    "od_model_dict = torch.load(model_path, map_location=torch.device(device))\n",
    "od_model.load_state_dict(od_model_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vcMJ6vYakwP"
   },
   "source": [
    "# Función de score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS5mOP-lff98",
    "outputId": "7be3675c-e1b8-4944-8864-97d723323d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 638 µs (started: 2022-10-14 15:04:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import urllib\n",
    "import math\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAPuxMfOZuSW",
    "outputId": "725adb2b-f909-4ff3-f731-a4fdf0f62188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.75 ms (started: 2022-10-05 12:53:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "# supuesto: tabla con enlaces a cada fotografía\n",
    "\n",
    "def score(url, classificator_1, classificator_2, detector, pesos = {'w_prod': 0.4, 'w_notface': 0.1,\n",
    "                                                 'w_label': 0.3, 'w_num': 0.1, 'w_contx': 0.2},\n",
    "          thresholds = {'t_prod': 0.5, 't_face': 0.5, 't_label': 0.5, 't_num': 0.5, 't_ctx_down': 0.2, \n",
    "                        't_ctx_up': 0.65}, device = 'cuda'):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "      url (str): url donde se encuentra la imagen\n",
    "      classificator_1 (modelo): clasificador multi-etiqueta para detectar la etiqueta del paquete\n",
    "      classificator_2 (modelo): clasificador multi-etiqueta para detectar la cara y el domicilio\n",
    "      detector (modelo): detector de objetos para identificar el paquete y su bbox respectivo\n",
    "      pesos (dict): diccionario con valores para cada peso, es decir,\n",
    "      w_prod (product), w_notface (without face), w_label (product label), \n",
    "      w_num (address number) y w_contx (context)\n",
    "      thresholds = diccionario con valores de umbral para cada criterio, es decir,\n",
    "      t_prod (product), t_face (face detector), t_label (product label) y \n",
    "      t_num (address number)\n",
    "      \n",
    "  Obs: los pesos deben sumar 1 para todos los criterios menos el del numero de domicilio. Este\n",
    "       ultimo corresponde a un beneficio de +0.1 si es que aparece en la fotografía.\n",
    "\n",
    "  Returns:\n",
    "      result_data (dataFrame): cada una de las columnas del dataFrame corresponde a la predicción\n",
    "      de cada criterio sobre cierto umbral, los scores (confianza del modelo) de cada criterio y \n",
    "      la nota de la foto (score).\n",
    "      t_img_0 (tensor): imagen procesada\n",
    "      b (array): bounding boxes de la imagen  \n",
    "  \"\"\"\n",
    "  # evaluar con gpu o cpu \n",
    "  device = torch.device(device)\n",
    "\n",
    "  classificator_1.to(device)\n",
    "  classificator_1.eval()  \n",
    "  classificator_2.to(device)\n",
    "  classificator_2.eval()           \n",
    "  detector.to(device)\n",
    "  detector.eval()\n",
    "  # transformación para clasificador\n",
    "  transformations = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                                 ])\n",
    "  with torch.no_grad():\n",
    "    # Obtener imagen\n",
    "    try:\n",
    "        url_open = urllib.request.urlopen(url)\n",
    "        image_cv = np.asarray(bytearray(url_open.read()), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image_cv, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # CLASIFICADOR MULTI-ETIQUETA\n",
    "        # transformación para aplicar clasificador\n",
    "        t_img = transformations(image)\n",
    "        img = t_img.unsqueeze(dim = 0)\n",
    "        # obtener clasificación\n",
    "        #---------------------------clasificador 1--------------------------------\n",
    "        label_base = classificator_1(img.to(device))\n",
    "        #---------------------------clasificador 2--------------------------------\n",
    "        label_etiqueta = classificator_2(img.to(device))\n",
    "        # obtener score\n",
    "        score = torch.sigmoid(label_base['label']).squeeze()\n",
    "        score_etiqueta = torch.sigmoid(label_etiqueta['label']).squeeze()\n",
    "        # analizar score para 'etiqueta'\n",
    "        s_etiqueta = score_etiqueta[0].item()\n",
    "        if s_etiqueta >= thresholds['t_label']:\n",
    "          etiqueta = 1\n",
    "        else:\n",
    "          etiqueta = 0\n",
    "        # analizar score para 'n° domicilio'\n",
    "        s_domicilio = score[1].item()\n",
    "        if s_domicilio >= thresholds['t_num']:\n",
    "          domicilio = 1\n",
    "        else:\n",
    "          domicilio = 0\n",
    "        # analizar score para 'cara'\n",
    "        s_cara = score[2].item()\n",
    "        s_no_cara = 1-s_cara\n",
    "        if s_cara >= thresholds['t_face']:\n",
    "          no_cara = 0\n",
    "        else:\n",
    "          no_cara = 1\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # DETECTOR DE OBJETOS\n",
    "        # transformación para aplicar detector\n",
    "        t_img_0 = transform.resize(image, (480, 480))\n",
    "        # obtener area de la imagen\n",
    "        area_img = t_img_0.shape[0]*t_img_0.shape[1]\n",
    "        t_img = torch.Tensor(t_img_0).permute(2,0,1).to(device)\n",
    "        od_prediction = od_model([t_img])\n",
    "        \n",
    "        # analizar score para 'paquete'\n",
    "        s_paquete = max(od_prediction[0]['scores']).item()\n",
    "        if od_prediction[0]['labels'].shape[0] > 0 and \\\n",
    "        s_paquete >= thresholds['t_prod']:\n",
    "          paquete = 1\n",
    "        else:\n",
    "          paquete = 0 \n",
    "        # determinar unión de todos los bbox que encierren a un paquete (score >= t_prod) \n",
    "        boxes = od_prediction[0]['boxes'].to(device)\n",
    "        scores = od_prediction[0]['scores'].to(device)\n",
    "        scores = torch.where(scores >= thresholds['t_prod'], 1, 0)\n",
    "        scores = torch.unsqueeze(scores, dim=0)\n",
    "        scores_t = torch.transpose(scores, 0, 1)\n",
    "        mult = torch.mul(boxes, scores_t)\n",
    "        b = mult[mult.sum(dim=1) != 0] # eliminar filas con puros ceros\n",
    "        l =list(range(b.size()[0]))\n",
    "        # recorrer cajas y calcular inter-area para cada combinación\n",
    "        indice = 0\n",
    "        interArea = 0\n",
    "        for i in itertools.combinations(l, r=2):\n",
    "          # obtener coordenadas de la inter-area\n",
    "          x0 = max(b[i[0]][0], b[i[1]][0])\n",
    "          y0 = max(b[i[0]][1], b[i[1]][1])\n",
    "          x1 = min(b[i[0]][2], b[i[1]][2])\n",
    "          y1 = min(b[i[0]][3], b[i[1]][3])\n",
    "    \n",
    "          # calcular inter-area\n",
    "          dif_x = x0-x1\n",
    "          dif_y = y0-y1\n",
    "          # se verifica que las esquinas de la interArea esten bien ubicadas \n",
    "          if dif_x < 0 and dif_y < 0:\n",
    "            interArea += dif_x*dif_y\n",
    "          ##interArea += abs(x0-x1)*abs(y0-y1)\n",
    "      \n",
    "        # sumar areas de cada bbox\n",
    "        area_total_bbox = 0\n",
    "        for box in b:\n",
    "          # calcular area de cada bbox\n",
    "          area_bbox = abs(box[0]-box[2])*abs(box[1]-box[3])\n",
    "          # calcular area total de bbox\n",
    "          area_total_bbox += area_bbox\n",
    "    \n",
    "        # calcular la union de las areas de cada bbox\n",
    "        union = area_total_bbox - interArea\n",
    "    \n",
    "        # calcular contexto\n",
    "        if torch.is_tensor(union):\n",
    "          contexto = union.item()/area_img\n",
    "      \n",
    "        else:\n",
    "          contexto = union/area_img\n",
    "        \n",
    "        # analizar contexto \n",
    "        if (contexto >= thresholds['t_ctx_down']) and (contexto <= thresholds['t_ctx_up']):\n",
    "          ctx_value = 1\n",
    "    \n",
    "        else:\n",
    "          ctx_value = 0\n",
    "      \n",
    "        # calcular score\n",
    "        score = pesos['w_prod']*paquete + pesos['w_label']*etiqueta + \\\n",
    "        pesos['w_notface']*no_cara + pesos['w_num']*domicilio + pesos['w_contx']*ctx_value\n",
    "        \n",
    "        enlace = 'correcto'\n",
    "    \n",
    "    except:\n",
    "        paquete = None\n",
    "        s_paquete = None\n",
    "        etiqueta = None\n",
    "        s_etiqueta = None\n",
    "        no_cara = None\n",
    "        s_no_cara = None\n",
    "        domicilio = None\n",
    "        s_domicilio = None\n",
    "        contexto = None\n",
    "        ctx_value = None\n",
    "        score = None\n",
    "        t_img_0 = None\n",
    "        b = None\n",
    "        enlace = 'incorrecto'\n",
    "        \n",
    "    # Vector de salida (fila) Prob. Etiqueta producto, Etiqueta producto, Prob. Numero domicilio, Numero domicilio\n",
    "    ##result = np.array([paquete, etiqueta, no_cara, domicilio, contexto, ctx_value, score])\n",
    "    result = np.array([paquete, s_paquete, etiqueta, s_etiqueta, no_cara, s_no_cara, domicilio,\\\n",
    "                       s_domicilio, contexto, ctx_value, score, enlace])\n",
    "    # Se hace un dataframe\n",
    "    ##result_data = pd.DataFrame([result], [0], columns=['paquete', 'etiqueta_producto', 'sin_rostro',\\\n",
    "    ##                                              'numero_domicilio', 'contexto', 'ctx_value', 'score'])\n",
    "    result_data = pd.DataFrame([result], [0], columns=['paquete', 's_paquete', 'etiqueta_producto', \\\n",
    "                                                       's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\\\n",
    "                                                       'numero_domicilio', 's_numero_domicilio', 'contexto',\\\n",
    "                                                       'ctx_value', 'score', 'enlace'])\n",
    "    \n",
    "    return result_data, t_img_0, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.72 ms (started: 2022-10-05 12:53:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def prediccion_por_sample(sample, j, dataset = 'image_recognition', full_table = 'prediction_table', \n",
    "               random_table = 'random_table', random_table_backup  = 'random_table_backup',\n",
    "              n=100, n_r=1):\n",
    "    import time\n",
    "    t_ini = time.time()\n",
    "    # Generar tabla \n",
    "    df_base = pd.DataFrame(columns = ['SOC', 'url', 'plate_num', 'provider_id', 'provider_verify_digit',\n",
    "                                      'provider_name', 'driver_id',\t'driver_verify_digit',\t\n",
    "                                      'driver_name', 'driver_last_name', 'event_crte_tmst',\n",
    "                                      'dfl_crte_tmst','paquete', 's_paquete', 'etiqueta_producto',\n",
    "                                      's_etiqueta_producto', 'sin_rostro', 's_sin_rostro',\n",
    "                                      'numero_domicilio', 's_numero_domicilio', 'contexto',\n",
    "                                      'ctx_value', 'score', 'enlace'])\n",
    "    \n",
    "    thres = {'t_prod': 0.5, 't_face': 0.6491, 't_label': 0.3, 't_num': 0.6, 't_ctx_down': 0.2, \n",
    "                            't_ctx_up': 0.65}\n",
    "    \n",
    "    largo_dataset = len(sample)\n",
    "    print('tamaño sample:',largo_dataset)\n",
    "    for i in range(len(sample)):\n",
    "      i+=j\n",
    "      url = sample.loc[[i]]['url'].item()\n",
    "      df1 = sample.loc[[i]].reset_index()\n",
    "      #aplicar modelo que retornara df2\n",
    "      df2, _, _ = score(url, ml_model_1, ml_model_2, od_model, thresholds = thres)\n",
    "      #df2, _, _ = score(url, ml_model_1, ml_model_2, od_model, thresholds = thres, device = 'cpu')\n",
    "      df3 = pd.concat((df1,df2), axis =1)\n",
    "      df_base = pd.concat((df_base,df3), ignore_index= True)\n",
    "      # revisión cada 100 imágenes \n",
    "      if i>0 and i%n==0:\n",
    "        print(i,'fotografías procesadas del total del dataset')      \n",
    "    t_fin = time.time()\n",
    "    delta_time = t_fin-t_ini \n",
    "    # Hacer copia del modelo\n",
    "    df_base_copy1 = df_base.copy()\n",
    "\n",
    "    # Cambiar tipo de dato de \"score\" a float\n",
    "    df_base_copy1[\"score\"] = pd.to_numeric(df_base_copy1[\"score\"])\n",
    "    # Seleccionar SOC que tiene mayor \"score\"\n",
    "    idx_max_score = df_base_copy1.groupby(['SOC'])['score'].transform(max) == df_base_copy1['score']\n",
    "    ##df_base_copy2 = df_base_copy1[idx_max_score].drop_duplicates(['SOC'])\n",
    "    # Agrupar por SOC y dejar indice\n",
    "    ##df_base_copy2 = df_base_copy1[['SOC', 'index', 'score']].groupby(['SOC'], sort = False).score.max()\n",
    "\n",
    "    # Agrupar por RUT, normalizar los nombres y dejar indice\n",
    "    #.agg(lambda x:x.value_counts().index[0])\n",
    "    df_base_copy3 = df_base_copy1[['provider_name', 'provider_id']].groupby(['provider_id'], sort = False)['provider_name'].transform('first').to_frame()\n",
    "    df_base_copy3['index'] = df_base_copy1['index']\n",
    "\n",
    "    # \"merge\" del dataset normalizado y el dataset original, con respecto al indice\n",
    "    df_base_copy4 = pd.merge(df_base_copy3, df_base_copy1, how='left', on='index')\n",
    "\n",
    "    # \"merge\" del dataset agrupado por SOC y el dataset anterior, con respecto al indice\n",
    "\n",
    "    ##df_base_final  = pd.concat([df_base_copy2, df_base_copy4], axis=1, join=\"inner\")\n",
    "    df_base_final = df_base_copy1[idx_max_score].drop_duplicates(['SOC'])\n",
    "\n",
    "    ##df_base_final = pd.merge(df_base_copy2, df_base_copy4, how='left', on='index')\n",
    "\n",
    "    # arreglar fechas\n",
    "    df_base_final['dfl_crte_tmst'] = df_base_final['dfl_crte_tmst'].dt.tz_localize(None)\n",
    "    df_base_final['event_crte_tmst'] = df_base_final['event_crte_tmst'].dt.tz_localize(None)\n",
    "    # guardar largo del dataset\n",
    "    new_df_base_final = df_base_final.assign(len_data = largo_dataset)\n",
    "    # guardar tiempo de ejecución\n",
    "    new_df_base_final = new_df_base_final.assign(execution_time_model = delta_time)\n",
    "    # generar tabla aleatoria\n",
    "    df_copy = new_df_base_final.copy()\n",
    "    df_filtrado = df_copy[(df_copy.enlace != 'incorrecto')]\n",
    "    df_filtrado=df_filtrado.assign(paquete_em=\"\", etiqueta_em=\"\", domicilio_em=\"\", rostro_em=\"\")\n",
    "    import pandas_gbq\n",
    "    # ubicación de destino para tabla procesada\n",
    "    destination_full_table = dataset + '.' + full_table\n",
    "    # anexar tabla de predicciones\n",
    "    pandas_gbq.to_gbq(new_df_base_final, destination_full_table, project_id='tc-sc-bi-bigdata-corp-tsod-dev', if_exists = 'append')\n",
    "    print(\"Se almacenó exitosamente tabla de predicciones\")\n",
    "    try:\n",
    "        # seleccionar pequeño conjunto de datos aleatorios de cada sample\n",
    "        # intentar generar tabla aleatoria\n",
    "        random_sample = df_filtrado.sample(n_r, frac=None, replace=False, weights=None, random_state=None)\n",
    "        # destino de tabla aleatoria\n",
    "        destination_random_table = dataset + '.' + random_table\n",
    "        # anexar tabla aleatoria a tabla generada durante el dia\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table, project_id='tc-sc-bi-bigdata-corp-tsod-dev', if_exists = 'append')\n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria\")\n",
    "        # anexar tabla aleatoria, con todas las tablas aleatorias generadas en dias anteriores\n",
    "        # para tenerlas de respaldo\n",
    "        destination_random_table_backup = dataset + '.' + random_table_backup\n",
    "        pandas_gbq.to_gbq(random_sample, destination_random_table_backup, project_id='tc-sc-bi-bigdata-corp-tsod-dev', if_exists = 'append')  \n",
    "        print(\"Se almacenó exitosamente sub-tabla aleatoria al backup\")\n",
    "    except:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.31 ms (started: 2022-10-05 12:53:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def predicciones_dataset(logger, fhandler, dataFrame=df_images, n_s=1000, n_i=100, n_r=200, dataset = 'image_recognition', \n",
    "                             full_table = 'prediction_table', random_table = 'random_table',\n",
    "                             random_table_backup  = 'random_table_backup', sample_partida = None):\n",
    "    # sample partida corresponde a el último sample que se pudo guardar \n",
    "    import math\n",
    "    # inicializar \n",
    "    count = 0\n",
    "    tamaño = len(dataFrame)\n",
    "    # numero de sub_samples por partición\n",
    "    n_s = \n",
    "    # escoger valores unicos de SOC\n",
    "    soc = df_images.SOC.unique()\n",
    "    # dividirlos en n_s subsamples\n",
    "    subsamples = np.array_split(soc, n_s)\n",
    "    # recorrer todos los subsamples\n",
    "    # obtener las filas correspondientes\n",
    "    # procesar cada fila con el modelo\n",
    "    for partition in subsamples:\n",
    "        # se obtienen las filas que estan en el subsample respectivo\n",
    "        df_images2 = df_images.loc[df_images.SOC.isin(partition)]\n",
    "    \n",
    "    \n",
    "    if tamaño/n_s <= n_r:\n",
    "        n_r2 = math.ceil(n_r/(tamaño/n_s))\n",
    "    else: \n",
    "        n_r2 = 1\n",
    "    if sample_partida:\n",
    "        ini = sample_partida*n_s\n",
    "        fin = ini\n",
    "        total_samples = math.ceil(tamaño/n_s)-sample_partida\n",
    "        partida = n_s*sample_partida+1\n",
    "    else:\n",
    "        ini = 0 \n",
    "        total_samples = math.ceil(tamaño/n_s)\n",
    "        partida = 0\n",
    "    # generar samples cada n elementos\n",
    "    termino=0\n",
    "    for n_filas in range(tamaño+1):\n",
    "        n_filas += partida\n",
    "        if n_filas > 0  and n_filas%n_s == 0:\n",
    "            fin = n_filas\n",
    "            sample = dataFrame.iloc[ini:fin]\n",
    "            prediccion_por_sample(sample,ini, dataset, full_table, \n",
    "               random_table, random_table_backup, n_i, n_r2)\n",
    "            ini+=n_s\n",
    "            count+=1\n",
    "            print(count, 'samples procesados de', total_samples)\n",
    "            ################################# logger\n",
    "            message = str(count) + ' sample(s) procesados de ' +  str(total_samples)\n",
    "            logging.info(message)\n",
    "        # generar ultimo sample en caso de que no se pueda dividir en partes enteras\n",
    "        if n_filas == tamaño and n_filas%n_s != 0:\n",
    "            sample = dataFrame.iloc[fin:n_filas]\n",
    "            prediccion_por_sample(sample,fin, dataset, full_table, random_table, \n",
    "                       random_table_backup, n_i, n_r2)\n",
    "            count+=1\n",
    "            termino = 1\n",
    "            print(count, 'samples procesados de', total_sample)\n",
    "            ################################# logger\n",
    "            message = str(count) + ' sample(s) procesados de ' +  str(total_samples)\n",
    "            logging.info(message)\n",
    "        if termino == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño sample: 3003\n",
      "500 fotografías procesadas del total del dataset\n",
      "1000 fotografías procesadas del total del dataset\n",
      "1500 fotografías procesadas del total del dataset\n",
      "2000 fotografías procesadas del total del dataset\n",
      "2500 fotografías procesadas del total del dataset\n",
      "3000 fotografías procesadas del total del dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente tabla de predicciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente sub-tabla aleatoria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11275.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se almacenó exitosamente sub-tabla aleatoria al backup\n",
      "1 samples procesados de 1\n",
      "time: 38min 16s (started: 2022-10-05 12:53:57 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import date\n",
    "##################################\n",
    "today = date.today()\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='/home/jupyter/Score/logs/k2_test_log_30_09_22.log', mode='a')\n",
    "fhandler.setLevel(logging.INFO)\n",
    "logger.addHandler(fhandler)\n",
    "logging.info('Fecha: ' + str(today))\n",
    "logging.info('Kernel: ' + str(2))\n",
    "\n",
    "predicciones_dataset(logger, fhandler, dataFrame=df_images2, n_s=len(df_images2), n_i=500, n_r=100, dataset = 'image_recognition', \n",
    "                             full_table = 'prediction_table_prueba_2', random_table = 'random_table_prueba_4',\n",
    "                             random_table_backup  = 'random_table_backup_prueba_2', sample_partida = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05887b1a958f4535938c3a9208b4ebaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9597f48902e54c7a9989484bcf6a239d",
      "placeholder": "​",
      "style": "IPY_MODEL_4a4aad02b8d24bffa799e6a5fdd4e006",
      "value": " 340M/340M [00:01&lt;00:00, 147MB/s]"
     }
    },
    "074323727f34416c895035b6d28e3738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a272d658182140b29fe755334af326c0",
      "placeholder": "​",
      "style": "IPY_MODEL_758e0c34b07e4c168617e8857e09b8fa",
      "value": " 160M/160M [00:01&lt;00:00, 90.2MB/s]"
     }
    },
    "1d7e20161e9248e6a41c541517122a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23c9ac6aff0448a2a69bf53e98c1b64a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32a0c7587cf74eeb98287fdf4767f938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e932cf1b4e484520a4dc4dd6672a7f17",
       "IPY_MODEL_e6dd759dedd84fe3afde7fafadbdc5e6",
       "IPY_MODEL_074323727f34416c895035b6d28e3738"
      ],
      "layout": "IPY_MODEL_43204103508c4482921d3f6bd45fe366"
     }
    },
    "3a331d5960ec443eb8064094c6f9a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b131ad046e94d0fa9405998f77117c1",
      "placeholder": "​",
      "style": "IPY_MODEL_719e9bde165b4db9816907ef904a8f67",
      "value": "100%"
     }
    },
    "3c468e9910384933988fb43f15ec7987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4271f826e55c4fee959a8097d9695e36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43204103508c4482921d3f6bd45fe366": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b4809efd1946ac911ccc2999bce65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a4aad02b8d24bffa799e6a5fdd4e006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bb2db3d2e644c26800558b8cdbc87fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a331d5960ec443eb8064094c6f9a414",
       "IPY_MODEL_a9e479cebc604561a5285a26a9b3cf9c",
       "IPY_MODEL_f94af2ae961249f1a2c6888009162afe"
      ],
      "layout": "IPY_MODEL_505388778fc745549e1be6b4478ada1b"
     }
    },
    "505388778fc745549e1be6b4478ada1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "719e9bde165b4db9816907ef904a8f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "758e0c34b07e4c168617e8857e09b8fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78b8a851199c421694811a3d2b09d77a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7b131ad046e94d0fa9405998f77117c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9597f48902e54c7a9989484bcf6a239d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97e08226970849feaea2b8c4a2c59162": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fb6091980f34f1fa8b8a6168760a397": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c5367310094e45baa5e4f21daa819a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a23ac5b3ea2d4bd9950f901288642e51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a272d658182140b29fe755334af326c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4d602abedbf472ba8443b587b7172a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e369973d094144a583efe831a749932e",
       "IPY_MODEL_f185edec20214d4f9174944db636ce7b",
       "IPY_MODEL_05887b1a958f4535938c3a9208b4ebaf"
      ],
      "layout": "IPY_MODEL_a23ac5b3ea2d4bd9950f901288642e51"
     }
    },
    "a9e479cebc604561a5285a26a9b3cf9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fb6091980f34f1fa8b8a6168760a397",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b4809efd1946ac911ccc2999bce65d",
      "value": 167502836
     }
    },
    "c084ae1be8e94b6da09dde8e90362301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d482f72576e946f59c20fea52f2973c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e369973d094144a583efe831a749932e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d482f72576e946f59c20fea52f2973c3",
      "placeholder": "​",
      "style": "IPY_MODEL_a0c5367310094e45baa5e4f21daa819a",
      "value": "100%"
     }
    },
    "e6dd759dedd84fe3afde7fafadbdc5e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23c9ac6aff0448a2a69bf53e98c1b64a",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d7e20161e9248e6a41c541517122a17",
      "value": 167502836
     }
    },
    "e932cf1b4e484520a4dc4dd6672a7f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4271f826e55c4fee959a8097d9695e36",
      "placeholder": "​",
      "style": "IPY_MODEL_97e08226970849feaea2b8c4a2c59162",
      "value": "100%"
     }
    },
    "f185edec20214d4f9174944db636ce7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3c586b991004e839c47ee46163195e8",
      "max": 356082095,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78b8a851199c421694811a3d2b09d77a",
      "value": 356082095
     }
    },
    "f3c586b991004e839c47ee46163195e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f94af2ae961249f1a2c6888009162afe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c084ae1be8e94b6da09dde8e90362301",
      "placeholder": "​",
      "style": "IPY_MODEL_3c468e9910384933988fb43f15ec7987",
      "value": " 160M/160M [00:01&lt;00:00, 121MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
